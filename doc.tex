
\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{graphicx}
\usepackage{makecell}
\usepackage{longtable}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{listings}
\geometry{margin=2.5cm}

\title{Preprocesamiento Basado en Textura para la Segmentación de Tumores en Ecografías Mamarias con MONAI}
\author{María del Mar Ávila, Juan del Junco, Nerea Jiménez\\Tutora: María José Jiménez}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
El resumen se hace al final del trabajo
\end{abstract}

\textbf{Palabras clave:} segmentación, MONAI, características de textura, U-Net, preprocesamiento, ecografía mamaria.

\section{Introducción}
El cáncer de mama representa una de las principales causas de mortalidad femenina. La segmentación precisa de tumores en imágenes de ecografía mamaria es esencial para el diagnóstico asistido por computadora. Sin embargo, estas imágenes presentan artefactos, bajo contraste y ruido speckle que dificultan la segmentación automática. Este trabajo propone comparar métodos de preprocesamiento tradicionales y texturales para mejorar la calidad de segmentación utilizando una red U-Net.

\section{Planteamiento Teórico}

La inteligencia artificial ha transformado numerosas disciplinas, y la medicina no es la excepción. En particular, la segmentación automática de imágenes médicas se ha vuelto una herramienta de gran valor en el diagnóstico clínico. Este proyecto se centra en el preprocesamiento y la segmentación de tumores en ecografías mamarias utilizando una red neuronal llamada U-Net, entrenada con ayuda de la librería especializada MONAI.

\subsection{Preprocesamiento}

El preprocesamiento de las imágenes antes del entrenamiento es esencial para mejorar la calidad del aprendizaje. En este trabajo, se aplican técnicas como el redimensionamiento, la normalización de intensidades y filtros tradicionales para la reducción de ruido. Esto permite estandarizar los datos de entrada, mejorando la estabilidad y efectividad del modelo entrenado.

En concreto y en el ámbito que nos compete, las imágenes médicas presentan numerosos desafíos: pueden estar en distintos tamaños, tener ruido o intensidades mal distribuidas. Por eso, antes de alimentar la red neuronal, se aplican procesos de preprocesamiento que pueden incluir:

\begin{itemize}
  \item \textbf{Redimensionamiento de imágenes:} El redimensionamiento de las imágenes a un tamaño uniforme es un paso inicial crucial. Las imágenes originales pueden tener diferentes resoluciones, por lo que unificarlas facilita el procesamiento y reduce la carga computacional. Se utilizan técnicas de interpolación (como bilineal o bicúbica) para mantener la calidad de la imagen al cambiar su tamaño.

  \item \textbf{Normalización de intensidades:} La normalización consiste en escalar los valores de intensidad de píxel a un rango común, como [0,1]. Esto mejora la estabilidad y la eficiencia del entrenamiento del modelo, ya que reduce la variabilidad entre imágenes. Comúnmente se utiliza la normalización min-max o la estandarización basada en media y desviación estándar.

  \item \textbf{Filtro Butterworth:} El filtro Butterworth es un filtro frecuencial que puede funcionar como pasa-bajos o pasa-altos. Su principal ventaja es que proporciona una transición suave en la frecuencia de corte, evitando artefactos bruscos. El filtro pasa-bajos suaviza la imagen eliminando el ruido de alta frecuencia, mientras que el pasa-altos resalta los bordes. La forma matemática del filtro asegura que no haya una caída abrupta, manteniendo una transición gradual.

  \item \textbf{Filtro de mediana adaptativa:} Este filtro elimina eficazmente el ruido impulsivo (sal y pimienta) adaptando dinámicamente el tamaño de la ventana de filtrado. Si una ventana pequeña no es suficiente para identificar correctamente la mediana de los píxeles, el filtro aumenta el tamaño de la ventana hasta encontrar una mediana adecuada. Así, preserva los bordes y detalles mientras elimina el ruido localizado.

  \item \textbf{Características estadísticas de primer orden:} Estas características describen la distribución de las intensidades de los píxeles de forma individual, sin considerar su posición. Incluyen métricas como la media, varianza, asimetría, curtosis y entropía. Estas propiedades permiten caracterizar la luminosidad y el contraste general de la imagen.

  \item \textbf{Características estadísticas de segundo orden:} Se enfocan en las relaciones espaciales entre los píxeles. Se obtienen, por ejemplo, mediante la matriz de co-ocurrencia de niveles de gris (GLCM), que mide con qué frecuencia aparecen combinaciones de valores de gris a cierta distancia y dirección. A partir de esta matriz se calculan métricas como contraste, homogeneidad, energía y correlación, que describen patrones de textura en la imagen.
\end{itemize}

\subsection{Redes Neuronales Convolucionales (CNN)} 
Una \textbf{red neuronal convolucional} (o \textit{Convolutional Neural Network}, CNN) es un tipo de red de aprendizaje profundo especialmente adecuada para procesar imágenes. A diferencia de las redes totalmente conectadas, las CNN emplean \textit{capas convolucionales} que aplican filtros sobre la imagen para extraer automáticamente características locales (por ejemplo, bordes, texturas) y \textit{capas de pooling} para reducir la resolución espacial, preservando las características más relevantes. Esto permite aprender representaciones jerárquicas: las primeras capas detectan patrones simples y las más profundas combinan estas características en patrones complejos propios del contenido de la imagen. En resumen, una CNN es una arquitectura capaz de \textbf{aprender directamente a partir de los datos} de imagen, identificando patrones visuales útiles para la tarea dada.

En nuestro proyecto se utiliza una CNN para llevar a cabo la segmentación automática de tumores en ecografías mamarias. La red convolucional aprende a reconocer, píxel a píxel, qué regiones de la imagen corresponden a tejido tumoral y cuáles a tejido normal o fondo, a partir de ejemplos anotados. Gracias a las convoluciones, la red puede detectar las características visuales sutiles de un tumor en una ecografía (como variaciones de textura o bordes difusos). Este enfoque automatizado reemplaza la necesidad de definir manualmente criterios de segmentación, permitiendo que el modelo ``aprenda'' a partir de los datos.

\subsection*{Segmentación Semántica en Ecografías Mamarias}  
La \textbf{segmentación semántica} es una tarea de visión por computador que consiste en asignar una etiqueta de clase a cada píxel de una imagen mediante un algoritmo de aprendizaje profundo. En otras palabras, el resultado es una máscara donde cada píxel está clasificado como perteneciente a cierta categoría. En nuestro caso, trabajamos con una segmentación binaria: cada píxel de la ecografía se clasifica como “tumor” o “no tumor”.

Aplicada a \textbf{ecografías mamarias}, permite delimitar de forma precisa las lesiones tumorales en imágenes de ultrasonido. Estas imágenes suelen tener bajo contraste y presencia de ruido, lo que dificulta su interpretación. Mediante segmentación automática, el sistema aprende de ejemplos anotados a identificar patrones propios de los tumores. El resultado es una máscara que marca su ubicación y forma, lo cual puede acelerar y mejorar el diagnóstico clínico.

Para evaluar la calidad de la segmentación, el proyecto implementa métricas como el \textit{coeficiente de Dice} (DSC) y el \textit{Índice de Jaccard} (IoU), calculadas en el archivo \texttt{metrics\_evaluation.py}. Estas métricas comparan la superposición entre la predicción del modelo y la máscara real.

\subsection*{Arquitectura U-Net: Codificación, Decodificación y Conexiones de Salto}  
La arquitectura \textbf{U-Net} fue diseñada específicamente para segmentación de imágenes médicas. Tiene una forma en “U” compuesta por dos fases:

\begin{itemize}
  \item \textbf{Codificación (encoder)}: reduce progresivamente el tamaño de la imagen aplicando convoluciones y submuestreo (stride o pooling), extrayendo características globales.
  \item \textbf{Decodificación (decoder)}: reconstruye la resolución original mediante upsampling, refinando la predicción en cada paso.
\end{itemize}

Un elemento clave son las \textbf{conexiones de salto} (\textit{skip connections}), que copian los mapas de características del encoder y los concatenan con los del decoder en cada nivel. Esto permite recuperar detalles finos que se habrían perdido en el proceso de compresión, logrando una segmentación precisa, especialmente en bordes y estructuras pequeñas como los tumores.

\subsection*{Configuración Específica de U-Net empleada en el Proyecto}
El modelo se construye con la clase \texttt{UNet} de la librería MONAI, con la siguiente configuración:

\begin{itemize}
  \item \textbf{Espacio 2D}: \texttt{spatial\_dims=2}, adecuado para imágenes planas.
  \item \textbf{Canales}: Imagen de entrada en escala de grises (\texttt{in\_channels=1}) y salida binaria (\texttt{out\_channels=1}).
  \item \textbf{Estructura de capas}: Cuatro niveles con \texttt{channels=(16, 32, 64, 128)} y \texttt{strides=(2,2,2)} para reducir resolución.
  \item \textbf{Bloques residuales}: Se emplean bloques de dos convoluciones por nivel (\texttt{num\_res\_units=2}) que mejoran la estabilidad del entrenamiento.
  \item \textbf{Activaciones}: ReLU en capas intermedias; sigmoide al final para producir probabilidades píxel a píxel.
  \item \textbf{Función de pérdida}: \texttt{DiceLoss} con activación sigmoide incorporada.
  \item \textbf{Entrenamiento}: 15 épocas, batch size 2, optimizador Adam, con imágenes redimensionadas a 256x256 píxeles.
\end{itemize}

Este modelo, entrenado con imágenes del dataset BUSI y sus respectivas máscaras, se guarda como \texttt{unet\_busi.pt} y luego puede utilizarse para segmentar nuevas ecografías. La arquitectura, gracias a su simetría y al uso de conexiones de salto, ofrece segmentaciones precisas incluso en imágenes complejas y ruidosas como las ecografías mamarias.

\subsection*{Framework MONAI}
MONAI (Medical Open Network for AI) es una librería desarrollada para facilitar la implementación de modelos de aprendizaje profundo en el campo de la medicina. Extiende las funcionalidades de PyTorch con herramientas especializadas, incluyendo:

\begin{itemize}
    \item Transformaciones específicas para imágenes médicas (carga de datos, normalización, redimensionamiento).
    \item Modelos predefinidos como U-Net, V-Net y otros.
    \item Funciones de pérdida y métricas de evaluación adaptadas a segmentación.
    \item Integración con formatos de imagen médica (DICOM, NIfTI).
\end{itemize}

Gracias a MONAI, el desarrollo de este proyecto se beneficia de un entorno robusto y reproducible, permitiendo un enfoque modular y escalable para el tratamiento de imágenes médicas.



\section{Implementación}
\subsection{Obtención y preparación del dataset}

Para llevar a cabo la tarea de segmentación de tumores en imágenes de ecografía mamaria, se ha utilizado el conjunto de datos público \texttt{Breast Ultrasound Images Dataset (Dataset BUSI)}, disponible en la página de Aly Fahmy\footnote{\href{https://scholar.cu.edu.eg/?q=afahmy/pages/dataset}{Dataset BUSI}}. Este dataset está compuesto por un total de 780 imágenes ecográficas divididas en tres categorías clínicas: \textit{benignas}, \textit{malignas} y \textit{normales}, junto con su correspondiente máscara de segmentación manual \cite{al2020dataset}.

Para el tratamiento de las imágenes del dataset, lo que se ha hecho en nuestro código ha sido, en primer lugar, organizar el conjunto de datos localmente en una estructura de carpetas separadas por clases. Y en segundo lugar, se estandariza el tamaño de las imágenes y sus máscaras, de forma que todas compartan las mismas dimensiones. Esta homogeneización se lleva a cabo con el objetivo de introducir los datos en el modelo de segmentación, ya que dicho modelo necesita entradas con dimensiones fijas.

Para hacer más fácil todo este proceso de carga, transformación e integración de datos en el entrenamiento de la red neuronal, se creó una clase llamada \texttt{BUSIDataset}. Esta clase se encuentra en el archivo \texttt{train\_unet\_monai.py}, y permite emparejar cada imagen con su máscara correspondiente, redimensionarlas, normalizarlas y convertirlas en tensores compatibles con PyTorch. Para más información sobre \texttt{BUSIDataset} ir a la sección \hyperref[sec:entrenamiento]{\texttt{Modelo de segmentación y entrenamiento}}, donde se explica con mayor detalle.

Como conclusión de este apartado, decir que, tanto la preparación de datos, como la clase \texttt{BUSIDataset}, han sido desarrolladas por nosotros mismos adaptándonos de forma específica a las necesidades de segmentación en imágenes de ecografía mamaria.

\subsection{Selección de herramientas}

Para el desarrollo de esta parte del código hemos usado varias librerías de Python:

\begin{itemize}

    \item \textbf{NumPy (numpy)} \cite{numpy}
    \begin{itemize}
        \item \textit{¿Qué es?:} Librería para trabajar con matrices y realizar operaciones matemáticas de forma rápida y eficiente en Python.
        \item \textit{¿Para qué la usamos?:} Para normalizar los valores de las imágenes, transformándolos de 0--255 a 0--1, realizar conversiones de tipos de datos y cálculos matemáticos como el producto o suma de matrices y la obtención de estadísticas necesarias para el preprocesamiento y la evaluación.
    \end{itemize}
    
    \item \textbf{OpenCV (cv2)} \cite{opencv}
    \begin{itemize}
        \item \textit{¿Qué es?:} Librería usada para el procesamiento de imágenes.
        \item \textit{¿Para qué la usamos?:} Para leer las imágenes y sus correspondientes máscaras desde archivos en formato PNG, redimensionar las imágenes a un tamaño fijo que pueda ser procesado por la red neuronal (por ejemplo, 256x256 píxeles), aplicar filtros clásicos (Butterworth y mediana), convertir imágenes a escala de grises y fusionar canales para crear imágenes RGB artificiales.
    \end{itemize}

    \item \textbf{PyTorch (torch)} \cite{pytorch}
    \begin{itemize}
        \item \textit{¿Qué es?:} Librería de deep learning utilizada para construir y entrenar redes neuronales.
        \item \textit{¿Para qué la usamos?:} Para definir tensores que representan imágenes y máscaras, crear el modelo de red neuronal, gestionar el entrenamiento mediante la retropropagación y actualización de pesos, mover los datos a GPU si está disponible, y guardar/cargar modelos entrenados.
    \end{itemize}

    \item \textbf{Matplotlib (matplotlib)} \cite{matplotlib}
    \begin{itemize}
        \item \textit{¿Qué es?:} Librería para crear gráficos y visualizar datos.
        \item \textit{¿Para qué la usamos?:} Para guardar comparativas visuales entre las imágenes originales, las máscaras reales y las predicciones generadas por el modelo. También se utiliza para representar los distintos pasos del preprocesamiento de forma gráfica.
    \end{itemize}

    \item \textbf{glob y os (glob, os)} \cite{glob}
    \cite{os}
    \begin{itemize}
        \item \textit{¿Qué son?:}
        \begin{itemize}
            \item \textit{glob} es un módulo que permite buscar archivos siguiendo patrones de nombre (por ejemplo, todos los archivos .png de una carpeta).
            \item \textit{os} es un módulo estándar de Python para trabajar con rutas de archivos y carpetas.
        \end{itemize}
        \item \textit{¿Para qué los usamos?:} Para buscar automáticamente las imágenes y sus máscaras dentro de las carpetas del dataset, recorrer directorios, crear nuevas carpetas para guardar resultados (como predicciones y comparaciones) y construir rutas de archivos dinámicamente.
    \end{itemize}

    \item \textbf{MONAI (monai)} \cite{monai}
    \begin{itemize}
        \item \textit{¿Qué es?:} MONAI (Medical Open Network for AI) es una librería construida sobre PyTorch, especializada en el procesamiento y análisis de imágenes médicas. Ha sido desarrollada en colaboración con expertos médicos y de inteligencia artificial para facilitar el desarrollo de modelos de Deep Learning en el ámbito médico.
        \item \textit{¿Qué ofrece MONAI?:} Modelos predefinidos específicamente diseñados para imágenes médicas (por ejemplo, U-Net, V-Net, SegResNet). Funciones de pérdida específicas para segmentación médica (como DiceLoss). Métricas de evaluación adaptadas a problemas de segmentación (como DiceMetric). Transformaciones y utilidades específicas para imágenes de tomografías, resonancias magnéticas, ecografías, etc. Herramientas para asegurar resultados reproducibles.
        \item \textit{¿Para qué la usamos?:}
        \begin{itemize}
            \item \textit{UNet:} Se utiliza para construir directamente el modelo de red neuronal de segmentación.
            \item \textit{DiceLoss:} Funciona como medida de error durante el entrenamiento, favoreciendo que las segmentaciones predichas se parezcan mucho a las reales.
            \item \textit{DiceMetric:} Permite calcular la calidad de las segmentaciones de forma precisa tras cada época de entrenamiento.
            \item \textit{set\_determinism:} Se usa para asegurar que los resultados sean reproducibles, fijando una semilla aleatoria en las operaciones.
        \end{itemize}
        \item \textit{¿Por qué se ha elegido MONAI?:} Aunque PyTorch podría cubrir las necesidades básicas, MONAI simplifica mucho el trabajo para tareas médicas:
        \begin{itemize}
            \item Reduce el número de líneas de código necesarias.
            \item Ofrece componentes optimizados y validados en aplicaciones médicas reales.
            \item Mejora la precisión y estabilidad de los modelos sin necesidad de implementaciones manuales complicadas.
        \end{itemize}
        Por tanto, gracias a MONAI, podemos centrarnos más en el preprocesamiento, la evaluación de los resultados y la comparativa entre usar o no el preprocesamiento desarrollado, que en programar de forma detallada la U-Net usada.
    \end{itemize}

    \item \textbf{scikit-image y scipy} \cite{skimage, scipy}
    \begin{itemize}
        \item \textit{¿Qué son?:} Librerías utilizadas para el análisis y procesamiento de imágenes científicas.
        \item \textit{¿Para qué las usamos?:} scikit-image se ha utilizado para calcular la entropía local en las texturas de primer orden. scipy se ha usado para aplicar transformadas de Fourier necesarias en el filtro Butterworth.
    \end{itemize}

    \item \textbf{scikit-learn} \cite{sklearn}
    \begin{itemize}
        \item \textit{¿Qué es?:} Librería usada para aprendizaje automático, también ofrece herramientas estadísticas.
        \item \textit{¿Para qué la usamos?:} Para calcular métricas de evaluación como la precisión, sensibilidad (recall), especificidad, matriz de confusión y exactitud de las predicciones generadas por el modelo en la fase de evaluación.
    \end{itemize}

\end{itemize}

\subsection{\texttt{Preprocesamiento de imágenes}}
En el preprocesamiento de imágenes, buscamos preparar los datos de entrada para mejorar la calidad de la segmentación. Para ello, hemos implementado diferentes técnicas para reducir el ruido, resaltar contornos y extraer información estructural. Todo esto se hace en la clase \texttt{preprocessing.py}: 

\begin{itemize}

    \item \textbf{butterworth\_high\_pass(img):} Esta función aplica un filtro pasa-altos de Butterworth, que atenúa las componentes de baja frecuencia y resalta los bordes al usar transformadas de Fourier e inversas \cite{mathworks_butterworth}. Este método ayuda a mejorar la visibilidad de los bordes tumorales difusos en las imágenes ecográficas.

    \item \textbf{adaptive\_median\_filter(img):} Esta función utiliza un filtro de mediana adaptativa para eliminar el ruido tipo sal y pimienta, ajustando dinámicamente el tamaño de la ventana de filtrado y preservando mejor los bordes que un filtro de media \cite{scribd_median_filter}.

    \item \textbf{first\_order\_features(img):} Extrae media local y entropía local de la imagen. La media suaviza regiones homogéneas y la entropía mide la complejidad local (cantidad de información) en la vecindad de cada píxel, útil para resaltar zonas de alta variabilidad como lesiones \cite{skimage_entropy}.

    \item \textbf{second\_order\_features\_fallback(img):} Emplea el detector de bordes de Canny y el filtro Laplaciano como alternativa a GLCM. Canny encuentra contornos mediante gradientes y supresión de no-máximos, mientras Laplaciano realza regiones de cambio abrupto de intensidad \cite{programarfacil_canny}.

    \item \textbf{fuse\_channels(ch1, ch2, ch3):} Combina tres canales en una imagen RGB artificial mediante fusión de matrices, permitiendo codificar en cada canal información distinta (intensidad, textura, bordes) para enriquecer la entrada del modelo.

    \item \textbf{process\_image(img\_path, output\_dir):} Aplica de forma ordenada todas las técnicas de preprocesamiento anteriores, guarda las versiones resultantes con sufijos descriptivos y genera una comparativa visual de los efectos de cada método de preprocesamiento.

\end{itemize}

El resultado es un conjunto de imágenes derivadas, una por técnica, que sirven de entrada a la red de segmentación. Esto permite comparar objetivamente el impacto de cada método en el rendimiento de detección de tumores.

\subsection{Modelo de segmentación y entrenamiento}
\label{sec:entrenamiento}
En esta parte del proyecto, buscamos construir y entrenar una red neuronal profunda para segmentar las imágenes de ecografía, tanto preprocesadas como sin preprocesar. Todo esto se implementa en el archivo \texttt{train\_unet\_monai.py}, y utiliza la arquitectura U-Net ofrecida por la librería MONAI \cite{monai_unet}.
Cabe destacar que usamos la arquitectura U-Net, porque es muy utilizada en el ámbito de la imagen médica debido a su capacidad para aprender tanto el contexto global como los detalles locales de la imagen mediante una estructura simétrica de codificador y decodificador con conexiones de salto. Estas conexiones permiten recuperar información espacial perdida durante las operaciones de reducción de resolución, lo cual es fundamental en tareas como la segmentación de tumores, donde los bordes y la localización precisa son críticos \cite{damavis_unet}.

\begin{itemize}

    \item \textbf{Definición del conjunto de datos (clase BUSIDataset):} Se crea una clase personalizada que extiende de \texttt{TorchDataset} (PyTorch) y recibe dos listas: rutas a imágenes y rutas a sus máscaras correspondientes. En el método \texttt{\_\_getitem\_\_} se leen ambas, se convierten a escala de grises y se redimensionan a tamaño fijo (256x256). Además, se normalizan los valores de píxeles entre 0 y 1 y se convierten en tensores \cite{pytorch_dataset_doc}.

    \item \textbf{Construcción del modelo (MONAI UNet):} Se define una red neuronal convolucional U-Net con bloques encoder-decoder y conexiones de salto, adaptada para segmentación binaria. La arquitectura tiene cuatro niveles de profundidad, canales crecientes (16 a 128), y unidades residuales en cada nivel. Está diseñada para trabajar con imágenes de entrada de un solo canal (escala de grises) y producir una única máscara binaria como salida \cite{monai_unet}.

    \item \textbf{Función de pérdida (DiceLoss):} Para entrenar el modelo, se utiliza la función \texttt{DiceLoss} provista por MONAI, una métrica común en segmentación médica que mide la similitud entre la máscara real y la predicha. Esta función penaliza especialmente los errores en clases desbalanceadas, como suelen ser los tumores respecto al fondo \cite{monai_dice_loss}.

    \item \textbf{Proceso de entrenamiento:} Durante 30 épocas, el modelo se entrena sobre lotes de imágenes cargadas mediante un \texttt{DataLoader} de MONAI. En cada iteración, se realiza una pasada hacia adelante, se calcula la pérdida con \texttt{DiceLoss}, se retropropaga el error y se actualizan los pesos usando el optimizador Adam. Se imprime la pérdida media por época para observar la convergencia \cite{pytorch_optim}.

    \item \textbf{Reproducibilidad (set\_determinism):} Se utiliza la función \texttt{set\_determinism()} de MONAI para fijar las semillas aleatorias de NumPy, PyTorch y MONAI, garantizando que los resultados sean reproducibles en diferentes ejecuciones \cite{monai_determinism}.

    \item \textbf{Guardado del modelo y visualización:} Una vez finalizado el entrenamiento, el modelo se guarda en disco y se generan predicciones de muestra sobre el mismo conjunto de entrenamiento. Las imágenes originales, las máscaras reales y las predichas se guardan en formato imagen para su comparación visual.

\end{itemize}

Gracias al uso de MONAI, esta parte del desarrollo se ha simplificado considerablemente en comparación con una implementación manual de U-Net desde cero, lo que ha permitido centrarse en el análisis del preprocesamiento y en la evaluación experimental.

\subsection{Evaluación de resultados}

Una vez que hemos entrenado el modelo de segmentación, es necesario evaluar su rendimiento para determinar si el preprocesamiento aplicado sobre las imágenes mejora, mantiene o incluso empeora los resultados obtenidos. Esta evaluación se ha hecho en el archivo \texttt{metrics\_evaluation.py}. Además, las métricas seleccionadas son las mismas que se usan en el estudio de referencia \cite{cai2022study}.

\begin{itemize}
    \item \textbf{Dice Similarity Coefficient (DSC):} Esta métrica evalúa la superposición entre dos conjuntos de píxeles segmentados. Toma valores entre 0 (no hay coincidencia) y 1 (coincidencia perfecta). Es especialmente útil en segmentación médica, donde las clases están desbalanceadas y el área tumoral es pequeña en relación al fondo.

    \item \textbf{IoU (Intersection over Union):} También conocida como Jaccard Index, mide la relación entre la intersección y la unión de los píxeles predichos y reales. Es más estricta que Dice y complementaria para analizar la calidad de segmentación.

    \item \textbf{Accuracy global:} Proporción de píxeles correctamente clasificados sobre el total de píxeles. Aunque es útil como referencia, puede resultar engañosa si la clase de fondo domina ampliamente sobre la clase tumoral.

    \item \textbf{Precisión (Precision):} Proporción de verdaderos positivos respecto al total de positivos predichos. Mide cuántas de las regiones predichas como tumor realmente lo son.

    \item \textbf{Sensibilidad (Recall):} También conocida como \textit{True Positive Rate} o \textit{Recall}, mide la proporción de píxeles tumorales correctamente detectados respecto al total real de píxeles tumorales. Es crítica para no omitir zonas afectadas.

    \item \textbf{Especificidad:} Mide la proporción de píxeles de fondo correctamente clasificados como tal. Es útil para asegurar que el modelo no sobresegmenta fuera de la región tumoral.

\end{itemize}

La evaluación se realiza recorriendo todas las máscaras predichas por el modelo y comparándolas con las correspondientes máscaras reales. Previamente, ambas imágenes se binarizan, redimensionan si es necesario y se comparan píxel a píxel. Las métricas se calculan usando funciones de \texttt{NumPy} y \texttt{scikit-learn}, y los resultados se promedian para ofrecer una visión general del rendimiento del modelo.

Esta evaluación se ha hecho con el objetivo principal de poder comparar los resultados obtenidos con diferentes versiones de imágenes (sin preprocesar, con filtros clásicos y con texturas), permitiendo ver de forma cuantitativa el efecto que cada técnica de preprocesamiento tiene sobre la segmentación automática.

\section{Experimentación}

\subsection{Objetivo de la experimentación}
El objetivo principal es evaluar el impacto de diferentes técnicas de \textbf{preprocesamiento} sobre el rendimiento de un modelo de segmentación de tumores en imágenes ecográficas. Se compara el rendimiento de la red U-Net entrenada con imágenes originales frente a aquellas que han sido sometidas a distintos métodos de realce y filtrado de textura.

\subsection{Configuración experimental}
Se ha utilizado el conjunto de datos \textbf{BUSI} (Breast Ultrasound Images), que contiene imágenes clasificadas en tres categorías: benignas, malignas y normales. La arquitectura empleada para la segmentación ha sido \textbf{U-Net}, entrenada con las siguientes condiciones constantes:
\begin{itemize}
    \item \textbf{Tamaño de entrada:} 256 × 256 píxeles
    \item \textbf{Épocas:} 30
    \item \textbf{Función de pérdida:} DiceLoss
    \item \textbf{Evaluación:} sobre imágenes originales y preprocesadas
\end{itemize}
La variable experimental es el \textbf{tipo de preprocesamiento} aplicado a las imágenes.

\subsection{Técnicas de preprocesamiento}
Se han evaluado cinco configuraciones distintas:
\begin{itemize}
    \item \textbf{Sin preprocesamiento:} imágenes originales del dataset.
    \item \textbf{Butterworth:} filtro pasa-altos en el dominio de la frecuencia, diseñado para resaltar bordes sin introducir ruido abrupto.
    \item \textbf{Filtro mediana adaptativa:} reduce el ruido tipo speckle característico de las imágenes ecográficas sin perder detalles de bordes.
    \item \textbf{Textura de primer orden:} basada en la media y entropía de vecindades locales, para representar intensidad y aleatoriedad.
    \item \textbf{Textura de segundo orden:} utiliza operadores como Canny y Laplaciano para aproximar características de bordes, autocorrelación y homogeneidad.
\end{itemize}

Las texturas se combinan con la imagen original formando una imagen \textbf{RGB tridimensional}, en la que cada canal representa una característica diferente (ej. canal rojo = original, verde = textura1, azul = textura2).

\subsection{Métricas de evaluación}
Se utilizan las siguientes métricas para evaluar el rendimiento del modelo:
\begin{itemize}
    \item \textbf{Dice Coefficient (DSC):} mide la superposición entre la máscara predicha y la real. Cuanto más alto, mejor (ideal: 1).
    \item \textbf{IoU (Intersection over Union):} relación entre la intersección y la unión de las máscaras predicha y real.
    \item \textbf{Accuracy:} proporción de píxeles correctamente clasificados (positivos y negativos).
    \item \textbf{Precision:} proporción de verdaderos positivos entre todos los positivos predichos.
    \item \textbf{Recall (Sensibilidad):} capacidad del modelo para detectar todos los píxeles positivos reales.
    \item \textbf{Specificity:} capacidad para identificar correctamente los píxeles negativos.
\end{itemize}

\subsection{Resultados}

Para valorar el alcance de los resultados obtenidos, se ha realizado una comparación directa con el estudio de Cai et al. (2022).
Ambos trabajos comparten el objetivo de mejorar la segmentación de tumores en imágenes de ecografía mamaria mediante técnicas de preprocesamiento, y utilizan arquitecturas de tipo \textbf{U-Net} para realizar la segmentación. Sin embargo, existen diferencias clave: mientras que el estudio de referencia implementa la red en TensorFlow, nuestro enfoque emplea la librería \textbf{MONAI}, desarrollada específicamente para aplicaciones clínicas sobre imágenes médicas. MONAI ofrece ventajas como transformaciones optimizadas, funciones de pérdida específicas como \texttt{DiceLoss}, métricas clínicas integradas y soporte directo para arquitecturas predefinidas, lo que permite un entrenamiento más estable y reproducible.

En la Tabla~\ref{tab:comparacion_estudio} se presentan los resultados comparativos. Se incluyen métricas fundamentales como el coeficiente de Dice (DSC), el índice de Jaccard (IoU), exactitud global (Accuracy), precisión, sensibilidad (Recall) y especificidad. Las técnicas evaluadas abarcan tanto filtros tradicionales (Butterworth, mediana) como métodos basados en características de textura de primer y segundo orden.

\begin{table}[H]
\centering
\caption{Comparativa de métricas entre nuestro trabajo y el estudio de Cai et al. (2022)}
\label{tab:comparacion_estudio}
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
\textbf{Técnica} & \textbf{DSC} & \textbf{IoU} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{Specificity} \\
\hline
\textbf{Ours - Sin preproc.}       & 0.630  & 0.530  & 0.930  & 0.630  & 0.680  & 0.960 \\
\textbf{Ours - Butterworth}        & 0.708  & 0.629  & 0.949  & 0.692  & 0.749  & 0.962 \\
\textbf{Ours - Mediana}            & 0.728  & 0.660  & 0.959  & 0.734  & 0.734  & 0.971 \\
\textbf{Ours - Textura 1er orden}  & 0.739  & \textbf{0.674}  & 0.956  & 0.730  & 0.757  & 0.965 \\
\textbf{Ours - Textura 2º orden}   & 0.735  & 0.668  & 0.958  & 0.726  & 0.758  & 0.969 \\
\hline
\textbf{Cai et al. - Butterworth HPF} & 0.720  & 0.479  & 0.859  & 0.587  & 0.743  & 0.872 \\
\textbf{Cai et al. - Mediana adapt.}  & 0.722  & 0.481  & 0.865  & 0.597  & 0.750  & 0.884 \\
\textbf{Cai et al. - Textura 1er ord.}& 0.744  & 0.562  & 0.945  & 0.641  & 0.775  & 0.956 \\
\textbf{Cai et al. - Textura 2º ord.} & \textbf{0.755}  & 0.602  & \textbf{0.976}  & \textbf{0.704}  & \textbf{0.803}  & \textbf{0.983} \\
\hline
\end{tabular}
\end{table}

También se han generado gráficas comparativas para visualizar el rendimiento de cada técnica en cada métrica, destacando la técnica ganadora por métrica.

\subsection{Análisis y conclusiones}

Los datos muestran que ambos enfoques coinciden en una conclusión clave: el preprocesamiento mejora de forma significativa la segmentación frente al uso de imágenes originales. Las técnicas basadas en textura —especialmente las de segundo orden— son las que alcanzan los mejores valores de DSC, IoU y sensibilidad, al capturar patrones estructurales relevantes en las imágenes ecográficas.

Comparando directamente, nuestros resultados con \textbf{textura de primer orden} logran un \textbf{IoU de 0.674}, que supera incluso al valor máximo alcanzado en el estudio de referencia (\textbf{0.602} con segundo orden), y presentan métricas de precisión y especificidad también más elevadas. Esto sugiere que el uso de MONAI ha permitido una integración más eficaz de las características texturales en el flujo de entrenamiento, posiblemente gracias a su gestión optimizada del pipeline médico y funciones de pérdida especializadas.
\\

En cuanto al \textbf{DSC}, el estudio alcanza un valor ligeramente superior (0.755) con segundo orden, mientras que nuestro valor máximo es 0.739. Sin embargo, la diferencia es estrecha, y nuestras métricas asociadas (precisión, IoU, especificidad) muestran un equilibrio general más alto.
\\

Por tanto, podemos concluir que el enfoque propuesto en este trabajo, basado en el uso de MONAI junto a preprocesamiento textural y arquitecturas U-Net, no solo confirma los hallazgos del estado del arte, sino que los mejora en aspectos clave como la superposición espacial (IoU) y la precisión global de la segmentación. Este resultado refuerza la aplicabilidad de MONAI como herramienta de referencia para tareas de segmentación médica avanzada.


\section{Manual de Usuario}
\subsection{Requisitos del sistema}
Para ejecutar el proyecto correctamente es necesario disponer de:

\begin{itemize}
    \item \textbf{Python 3.8} o superior.
    \item \textbf{Sistema operativo}: Windows, Linux o macOS.
    \item \textbf{Hardware recomendado}:
    \begin{itemize}
        \item CPU moderna.
        \item (Opcional) GPU compatible con CUDA para acelerar el entrenamiento (recomendado).
    \end{itemize}
\end{itemize}

\subsection{Instalación de dependencias}

Antes de ejecutar el código, se deben instalar las librerías necesarias.  
Desde la carpeta raíz del proyecto, ejecutar el siguiente comando:

\begin{lstlisting}[language=bash]
pip install -r requirements.txt
\end{lstlisting}

Esto instalará automáticamente las dependencias necesarias:
\begin{itemize}
    \item numpy
    \item opencv-python
    \item matplotlib
    \item scikit-image
    \item scipy
    \item torch
    \item monai
\end{itemize}

\textbf{Nota}: Si se dispone de una GPU y se desea aprovechar, se recomienda instalar \texttt{torch} con soporte CUDA siguiendo las instrucciones de \url{https://pytorch.org/}.

\subsection{Estructura del proyecto}

\begin{itemize}
    \item \texttt{main.py}: Script principal para lanzar las distintas fases del proyecto.
    \item \texttt{preprocessing.py}: Funciones de preprocesamiento de imágenes.
    \item \texttt{train\_unet\_monai.py}: Entrenamiento del modelo U-Net.
    \item \texttt{predict\_with\_unet.py}: Predicción de máscaras sobre nuevas imágenes.
    \item \texttt{metrics\_evaluation.py}: Evaluación de las predicciones mediante métricas.
    \item \texttt{unet\_busi.pt}: Archivo que guarda el modelo U-Net entrenado.
\end{itemize}

\subsection{Cómo utilizar el proyecto}

\subsubsection{Preprocesamiento de imágenes}

Para aplicar los filtros y generar imágenes procesadas, ejecutar:

\begin{lstlisting}[language=bash]
python main.py classic
\end{lstlisting}

Esto aplicará:
\begin{itemize}
    \item Filtro de Butterworth.
    \item Filtro de mediana.
    \item Extracción de características de primer orden (media y entropía).
    \item Extracción de características de segundo orden (bordes y cambios locales).
\end{itemize}

Los resultados se guardarán en la carpeta de salida especificada.

\subsubsection{Entrenamiento del modelo U-Net}

Para entrenar el modelo desde cero, ejecutar:

\begin{lstlisting}[language=bash]
python main.py train
\end{lstlisting}

Este comando:
\begin{itemize}
    \item Preparará el conjunto de datos (imágenes y máscaras).
    \item Definirá y entrenará el modelo U-Net.
    \item Guardará el modelo entrenado en el archivo \texttt{unet\_busi.pt}.
\end{itemize}

\subsubsection{Predicción con el modelo entrenado}

Para realizar predicciones sobre nuevas imágenes:

\begin{lstlisting}[language=bash]
python main.py predict
\end{lstlisting}

Esto:
\begin{itemize}
    \item Cargará el modelo entrenado.
    \item Aplicará la predicción a nuevas imágenes.
    \item Guardará las máscaras predichas y comparativas visuales.
\end{itemize}

\subsubsection{Evaluación de resultados}

Para evaluar la calidad de las predicciones:

\begin{lstlisting}[language=bash]
python main.py evaluate
\end{lstlisting}

Se calcularán automáticamente las métricas:
\begin{itemize}
    \item Dice Coefficient (DSC)
    \item Intersection over Union (IoU)
    \item Accuracy
    \item Precision
    \item Recall (Sensibilidad)
    \item Specificity
\end{itemize}

\subsection{Parámetros configurables}

El proyecto puede ajustarse mediante la modificación de ciertos parámetros definidos en el código. Estos son los parámetros más relevantes, su función, valor por defecto, ubicación en el código y consideraciones en caso de que se desee modificarlos:

\begin{itemize}
    \item \textbf{Número de épocas de entrenamiento (EPOCHS)}\\
    \textit{Archivo:} \texttt{train\_unet\_monai.py} \\
    \textit{Ubicación:} línea que contiene \texttt{EPOCHS = 30} \\
    \textit{Descripción:} define el número total de pasadas completas del conjunto de datos por la red neuronal durante el entrenamiento. Aumentarlo puede mejorar la precisión del modelo, pero también incrementa el tiempo de entrenamiento y riesgo de sobreajuste.

    \item \textbf{Tamaño del batch (BATCH\_SIZE)}\\
    \textit{Archivo:} \texttt{train\_unet\_monai.py} \\
    \textit{Descripción:} cantidad de imágenes procesadas simultáneamente antes de actualizar los pesos del modelo. Valores mayores pueden acelerar el entrenamiento si se dispone de memoria suficiente en GPU.

    \item \textbf{Tamaño de imagen (IMG\_SIZE)}\\
    \textit{Archivos:} \texttt{train\_unet\_monai.py}, \texttt{predict\_with\_unet.py} \\
    \textit{Descripción:} las imágenes de entrada y sus máscaras se redimensionan a este tamaño. Modificarlo puede afectar a la precisión y tiempo de entrenamiento. Por defecto es \texttt{(256, 256)}.

    \item \textbf{Tipo de preprocesamiento (PREPROC\_TAG)}\\
    \textit{Archivos:} \texttt{train\_unet\_monai.py}, \texttt{predict\_with\_unet.py}, \texttt{metrics\_evaluation.py} \\
    \textit{Descripción:} cadena que identifica qué tipo de imagen preprocesada se va a utilizar (\texttt{\_butterworth}, \texttt{\_median}, \texttt{\_first\_order}, etc.). Este sufijo debe coincidir con los nombres de archivos de imagen generados.

    \item \textbf{Rutas de entrada y salida} \\
    \textit{Archivo:} \texttt{main.py} \\
    \textit{Parámetros:} \texttt{test\_dir}, \texttt{output\_dir}, \texttt{gt\_dir\_root}, \texttt{pred\_dir} \\
    \textit{Descripción:} determinan dónde buscar las imágenes originales, las máscaras de ground truth y dónde guardar los resultados de las predicciones o evaluaciones.

    \item \textbf{Modo de ejecución} \\
    \textit{Archivo:} \texttt{main.py} \\
    \textit{Descripción:} se controla con el valor de la variable \texttt{mode}. Los valores posibles son:
    \begin{itemize}
        \item \texttt{"classic"}: aplicar preprocesamiento a todas las imágenes.
        \item \texttt{"train"}: entrenar el modelo desde cero.
        \item \texttt{"predict"}: generar predicciones con el modelo entrenado.
        \item \texttt{"evaluate"}: calcular métricas a partir de predicciones y máscaras reales.
    \end{itemize}
    \textit{Nota:} cualquier otro valor provocará un mensaje de error.
\end{itemize}

\textbf{Recomendación:} antes de modificar cualquier parámetro, se aconseja realizar una copia de seguridad del archivo correspondiente y asegurarse de mantener la coherencia en las rutas y nombres utilizados en distintas fases del proyecto.

\section{Conclusiones}

Este proyecto ha tenido como objetivo mejorar la segmentación automática de tumores en imágenes de ecografía mamaria, aplicando diferentes técnicas de preprocesamiento basadas en textura y usando una red neuronal U-Net entrenada con la librería MONAI. Las ecografías son un tipo de imagen médica especialmente difícil de procesar debido al ruido, al bajo contraste y a la variabilidad entre pacientes. Por eso, era importante estudiar si aplicar transformaciones antes del entrenamiento podía ayudar a obtener mejores resultados.

Después de entrenar la red con imágenes originales y con versiones preprocesadas, se evaluaron los resultados usando métricas estándar como el coeficiente de Dice (DSC), el índice de Jaccard (IoU), la precisión, la sensibilidad (recall), la especificidad y la exactitud general. En todos los casos, los resultados fueron mejores cuando se aplicó preprocesamiento, especialmente con las técnicas de textura de primer y segundo orden. Estas transformaciones ayudaron a resaltar información importante como los bordes del tumor o zonas con mayor variación de intensidad.

Comparando con estudios anteriores, nuestro sistema logró resultados similares o incluso mejores en métricas como el IoU y la precisión, especialmente gracias al uso de MONAI. Esta librería, pensada para imágenes médicas, permitió construir y entrenar la red U-Net de forma más eficiente, reproducible y con menos código. MONAI nos permitió centrarnos en el análisis de datos y en la mejora de la segmentación, sin tener que preocuparnos tanto por los detalles técnicos de bajo nivel.

En resumen, hemos comprobado que aplicar preprocesamiento basado en textura mejora la segmentación de tumores respecto a no aplicar ningún tratamiento previo. Además, el uso de herramientas especializadas como MONAI facilita el desarrollo de sistemas de segmentación robustos y eficaces. Estos resultados son prometedores y abren la puerta a aplicar esta misma estrategia en otros tipos de imágenes médicas, como resonancias o TACs.

Como posibles líneas futuras, se podrían probar otras arquitecturas más avanzadas (como Attention U-Net), aplicar aumentos de datos (data augmentation), trabajar con imágenes en 3D o incluso probar el sistema en entornos clínicos reales, validando los resultados con profesionales médicos.

\section{Autoevaluacion de cada miembro}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{ } & \textbf{Juan} & \textbf{Mar} & \textbf{Nerea} \\
\hline
\makecell{Comprensión \\ y dominio} & Excelente & Excelente & Excelente \\
\hline
\makecell{Exposición \\ didáctica} & Excelente & Excelente & Excelente \\
\hline
\makecell{Integración \\ del equipo} & Excelente & Excelente & Excelente \\
\hline
Objetivos & Excelente & Excelente & Excelente \\
\hline
\makecell{Aspectos \\ didácticos} & Excelente & Excelente & Excelente \\
\hline
\makecell{Experimentación \\ y conclusiones} & Excelente & Excelente & Excelente \\
\hline
Contenidos & Excelente & Excelente & Excelente \\
\hline
\makecell{Divulgación de los \\ contenidos} & Excelente & Excelente & Excelente \\
\hline
\makecell{Bibliografía. \\ Recursos científicos} & Excelente & Excelente & Excelente \\
\hline
\end{tabular}
\caption{Evaluación entre compañeros}
\end{table}

\section{Tabla de tiempos}

\begin{longtable}{|c|c|c|c|}
\hline
\makecell{Fecha de la \\ actividad} 
& \makecell{Tiempo \\ (h)} 
& Miembro 
& \makecell{Actividad \\ realizada} \\
\hline
\endfirsthead

\hline
\makecell{Fecha de la \\ actividad} 
& \makecell{Tiempo \\ (h)} 
& Miembro 
& \makecell{Actividad \\ realizada} \\
\hline
\endhead

\hline
\endfoot

18/02/2025 & 2 h 06 min & Nerea & \makecell{Búsqueda de tema y \\ referencia bibliográfica} \\
\hline
18/02/2025 & 11 min & Nerea & \makecell{Búsqueda y revisión \\ de repositorio} \\
\hline
25/02/2025 & 1 h 28 min & Nerea & \makecell{Creación, redacción y \\ revisión del documento \\ para el Entregable 1} \\
\hline
30/02/2025 & 2 h 39 min & Nerea & \makecell{Investigación sobre \\ los tumores mamarios} \\
\hline
10/03/2025 & 1 h 47 min & Nerea & \makecell{Revisión y análisis del \\ documento .ipynb \\ (primer script)} \\
\hline
\makecell{26/02/2025 \\ 4/03/2025 y 11/03/2025} & 5 h 30 min & Nerea & \makecell{Clases dedicadas \\ al proyecto} \\
\hline
11/03/2025 & 20 min & Nerea & \makecell{Creación, redacción y \\ revisión del documento \\ para el Entregable 2} \\
\hline
\makecell{1/04/2025 \\ 4/04/2025 y 7/04/2025} & 7 h 37 min & Nerea & \makecell{Investigación sobre MONAI \\ y redes U-net} \\
\hline
7/04/2025 y 8/04/2025 & 3 h 24 min & Nerea & \makecell{Redacción del documento \\ LaTeX: primer borrador} \\
\hline
\makecell{1/04/2025, \\ 2/04/2025 \\ 5/04/2025 y \\ 8/04/2025} & 10 h 26 min & Nerea & \makecell{Implementación seguimiento 3: \\ revisión de métodos del \\ preprocesamiento y \\ entrenamiento de la U-net} \\
\hline
\makecell{1/04/2025 \\ 5/04/2025 y 8/04/2025} & 6 h 11 min & Nerea & \makecell{Implementación seguimiento 3: \\ pruebas de ajustes del modelo} \\
\hline
8/04/2025 & 1 h 02 min & Nerea & \makecell{Creación, redacción y \\ revisión del documento \\ para el Entregable 3} \\
\hline
29/04/2025 & 1 h 32 min & Nerea & \makecell{Creación, redacción y \\ revisión del documento \\ para el Entregable 4} \\
\hline
\makecell{18/04/2025 \\ 28/04/2025} & 3 h 24 min & Nerea & \makecell{Revisión de correcciones de la profesora \\ y resolución de dudas internas sobre \\ el proyecto y MONAI} \\
\hline
\makecell{18/04/2025 \\ 20/04/2025 \\ 28/04/2025} & 6 h 36 min & Nerea & \makecell{Redacción y corrección del documento \\ LaTeX con base en la implementación, \\ información aprendida e investigación} \\
\hline
29/04/2025 & 2 h & Nerea & \makecell{Presentación: borrador} \\
\hline
\makecell{11/05/2025 \\ 12/05/2025} & 4 h 45 min & Nerea & \makecell{Revisión de documento LaTex, \\ pulir lo escrito y preparar \\ para el entregable \\ y guión} \\
\hline
\makecell{11/05/2025 \\ 12/05/2025} & 5 h 38 min ?? & Nerea & \makecell{Presentación: elaboración de diapositivas \\ y guión} \\
\hline
\caption{Tiempo dedicado al proyecto}
\end{longtable}


\begin{thebibliography}{9}

\bibitem{al2020dataset}
Fahmy, A. (s.f.). \textit{Dataset}. [Online]. Disponible en: \url{https://scholar.cu.edu.eg/?q=afahmy/pages/dataset}


\bibitem{numpy}
NumPy Developers, \textit{NumPy Documentation}, [Online]. Disponible en: \url{https://numpy.org/doc/stable/}

\bibitem{opencv}
OpenCV Team, \textit{OpenCV Documentation}, [Online]. Disponible en: \url{https://docs.opencv.org/4.x/index.html}

\bibitem{pytorch}
PyTorch Developers, \textit{PyTorch Documentation}, [Online]. Disponible en: \url{https://pytorch.org/docs/stable/index.html}

\bibitem{matplotlib}
Matplotlib Developers, \textit{Using Matplotlib}, [Online]. Disponible en: \url{https://matplotlib.org/stable/contents.html}

\bibitem{glob}
Python Software Foundation, \textit{glob — Unix style pathname pattern expansion}, [Online]. Disponible en: \url{https://docs.python.org/3/library/glob.html}

\bibitem{os}
Python Software Foundation, \textit{os — Miscellaneous operating system interfaces}, [Online]. Disponible en: \url{https://docs.python.org/3/library/os.html}

\bibitem{monai}
MONAI Consortium, \textit{Project MONAI}, [Online]. Disponible en: \url{https://docs.monai.io/}

\bibitem{skimage}
scikit-image Developers, \textit{scikit-image Documentation}, [Online]. Disponible en: \url{https://scikit-image.org/docs/stable/}

\bibitem{scipy}
SciPy Community, \textit{SciPy Documentation}, [Online]. Disponible en: \url{https://docs.scipy.org/doc/scipy/}

\bibitem{sklearn}
scikit-learn Developers, \textit{User Guide}, [Online]. Disponible en: \url{https://scikit-learn.org/stable/user_guide.html}

\bibitem{mathworks_butterworth}
MathWorks, \textit{Filtro paso alto}, [Online]. Disponible en: \url{https://es.mathworks.com/discovery/high-pass-filter.html}

\bibitem{scribd_median_filter}
J. C. Gutiérrez López, \textit{Filtro de Mediana Adaptativo}, Scribd, [Online]. Disponible en: \url{https://es.scribd.com/document/43929424/Filtro-de-Mediana-Adaptativo}

\bibitem{skimage_entropy}
scikit-image, \textit{Entropy}, [Online]. Disponible en: \url{https://scikit-image.org/docs/stable/auto_examples/filters/plot_entropy.html}

\bibitem{programarfacil_canny}
Programar Fácil, \textit{Detector de bordes Canny, cómo contar objetos con OpenCV y Python}, [Online]. Disponible en: \url{https://programarfacil.com/blog/vision-artificial/detector-de-bordes-canny-opencv/}

\bibitem{monai_unet}
MONAI Consortium, \textit{UNet - MONAI Network Architectures}, [Online]. Disponible en: \url{https://docs.monai.io/en/stable/networks.html#unet}

\bibitem{damavis_unet}
P. Alzamora, \textit{Segmentación semántica de imágenes con Deep Learning}, Damavis Blog, 10 de marzo de 2023. [Online]. Disponible en: \url{https://blog.damavis.com/segmentacion-semantica-de-imagenes-con-deep-learning/}

\bibitem{pytorch_dataset_doc}
PyTorch Developers, \textit{Writing Custom Datasets, DataLoaders and Transforms}, [Online]. Disponible en: \url{https://pytorch.org/tutorials/beginner/data_loading_tutorial.html}

\bibitem{monai_dice_loss}
MONAI Consortium, \textit{Loss Functions - DiceLoss}, [Online]. Disponible en: \url{https://docs.monai.io/en/stable/losses.html#diceloss}

\bibitem{pytorch_optim}
PyTorch Developers, \textit{torch.optim — Optimizers}, [Online]. Disponible en: \url{https://pytorch.org/docs/stable/optim.html}

\bibitem{monai_determinism}
MONAI Consortium, \textit{Utilities}, [Online]. Disponible en: \url{https://docs.monai.io/en/stable/utils.html#monai.utils.set_determinism}

\bibitem{cai2022study}
Cai, S., Zhang, J., Zhu, Y., \& Liu, T. (2022). "A Study on the Combination of Image Preprocessing Method Based on Texture Feature and Segmentation Algorithm for Breast Ultrasound Images". In: \textit{2022 2nd International Conference on Consumer Electronics and Computer Engineering (ICCECE)}, IEEE.



\end{thebibliography}

\end{document}
