

\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{listings}
\geometry{margin=2.5cm}

\title{Preprocesamiento Basado en Textura para la Segmentación de Tumores en Ecografías Mamarias con MONAI}
\author{María del Mar Ávila, Juan del Junco, Nerea Jiménez\\Tutora: María José Jiménez}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
El resumen se hace al final del trabajo
\end{abstract}

\textbf{Palabras clave:} segmentación, MONAI, características de textura, U-Net, preprocesamiento, ecografía mamaria.

\section{Introducción}
El cáncer de mama representa una de las principales causas de mortalidad femenina. La segmentación precisa de tumores en imágenes de ecografía mamaria es esencial para el diagnóstico asistido por computadora. Sin embargo, estas imágenes presentan artefactos, bajo contraste y ruido speckle que dificultan la segmentación automática. Este trabajo propone comparar métodos de preprocesamiento tradicionales y texturales para mejorar la calidad de segmentación utilizando una red U-Net.

\section{Planteamiento Teórico}

La inteligencia artificial ha transformado numerosas disciplinas, y la medicina no es la excepción. En particular, la segmentación automática de imágenes médicas se ha vuelto una herramienta de gran valor en el diagnóstico clínico. Este proyecto se centra en el preprocesamiento y la segmentación de tumores en ecografías mamarias utilizando una red neuronal llamada U-Net, entrenada con ayuda de la librería especializada MONAI.

\subsection{Preprocesamiento}

El preprocesamiento de las imágenes antes del entrenamiento es esencial para mejorar la calidad del aprendizaje. En este trabajo, se aplican técnicas como el redimensionamiento, la normalización de intensidades y filtros tradicionales para la reducción de ruido. Esto permite estandarizar los datos de entrada, mejorando la estabilidad y efectividad del modelo entrenado.

En concreto y en el ámbito que nos compete, las imágenes médicas presentan numerosos desafíos: pueden estar en distintos tamaños, tener ruido o intensidades mal distribuidas. Por eso, antes de alimentar la red neuronal, se aplican procesos de preprocesamiento que pueden incluir:

\begin{itemize}
  \item \textbf{Redimensionamiento de imágenes:} El redimensionamiento de las imágenes a un tamaño uniforme es un paso inicial crucial. Las imágenes originales pueden tener diferentes resoluciones, por lo que unificarlas facilita el procesamiento y reduce la carga computacional. Se utilizan técnicas de interpolación (como bilineal o bicúbica) para mantener la calidad de la imagen al cambiar su tamaño.

  \item \textbf{Normalización de intensidades:} La normalización consiste en escalar los valores de intensidad de píxel a un rango común, como [0,1]. Esto mejora la estabilidad y la eficiencia del entrenamiento del modelo, ya que reduce la variabilidad entre imágenes. Comúnmente se utiliza la normalización min-max o la estandarización basada en media y desviación estándar.

  \item \textbf{Filtro Butterworth:} El filtro Butterworth es un filtro frecuencial que puede funcionar como pasa-bajos o pasa-altos. Su principal ventaja es que proporciona una transición suave en la frecuencia de corte, evitando artefactos bruscos. El filtro pasa-bajos suaviza la imagen eliminando el ruido de alta frecuencia, mientras que el pasa-altos resalta los bordes. La forma matemática del filtro asegura que no haya una caída abrupta, manteniendo una transición gradual.

  \item \textbf{Filtro de mediana adaptativa:} Este filtro elimina eficazmente el ruido impulsivo (sal y pimienta) adaptando dinámicamente el tamaño de la ventana de filtrado. Si una ventana pequeña no es suficiente para identificar correctamente la mediana de los píxeles, el filtro aumenta el tamaño de la ventana hasta encontrar una mediana adecuada. Así, preserva los bordes y detalles mientras elimina el ruido localizado.

  \item \textbf{Características estadísticas de primer orden:} Estas características describen la distribución de las intensidades de los píxeles de forma individual, sin considerar su posición. Incluyen métricas como la media, varianza, asimetría, curtosis y entropía. Estas propiedades permiten caracterizar la luminosidad y el contraste general de la imagen.

  \item \textbf{Características estadísticas de segundo orden:} Se enfocan en las relaciones espaciales entre los píxeles. Se obtienen, por ejemplo, mediante la matriz de co-ocurrencia de niveles de gris (GLCM), que mide con qué frecuencia aparecen combinaciones de valores de gris a cierta distancia y dirección. A partir de esta matriz se calculan métricas como contraste, homogeneidad, energía y correlación, que describen patrones de textura en la imagen.
\end{itemize}

\subsection{Redes Neuronales Convolucionales (CNN)} 
Una \textbf{red neuronal convolucional} (o \textit{Convolutional Neural Network}, CNN) es un tipo de red de aprendizaje profundo especialmente adecuada para procesar imágenes. A diferencia de las redes totalmente conectadas, las CNN emplean \textit{capas convolucionales} que aplican filtros sobre la imagen para extraer automáticamente características locales (por ejemplo, bordes, texturas) y \textit{capas de pooling} para reducir la resolución espacial, preservando las características más relevantes. Esto permite aprender representaciones jerárquicas: las primeras capas detectan patrones simples y las más profundas combinan estas características en patrones complejos propios del contenido de la imagen. En resumen, una CNN es una arquitectura capaz de \textbf{aprender directamente a partir de los datos} de imagen, identificando patrones visuales útiles para la tarea dada.

En nuestro proyecto se utiliza una CNN para llevar a cabo la segmentación automática de tumores en ecografías mamarias. La red convolucional aprende a reconocer, píxel a píxel, qué regiones de la imagen corresponden a tejido tumoral y cuáles a tejido normal o fondo, a partir de ejemplos anotados. Gracias a las convoluciones, la red puede detectar las características visuales sutiles de un tumor en una ecografía (como variaciones de textura o bordes difusos). Este enfoque automatizado reemplaza la necesidad de definir manualmente criterios de segmentación, permitiendo que el modelo ``aprenda'' a partir de los datos.

\subsection*{Segmentación Semántica en Ecografías Mamarias}  
La \textbf{segmentación semántica} es una tarea de visión por computador que consiste en asignar una etiqueta de clase a cada píxel de una imagen mediante un algoritmo de aprendizaje profundo. En otras palabras, el resultado es una máscara donde cada píxel está clasificado como perteneciente a cierta categoría. En nuestro caso, trabajamos con una segmentación binaria: cada píxel de la ecografía se clasifica como “tumor” o “no tumor”.

Aplicada a \textbf{ecografías mamarias}, permite delimitar de forma precisa las lesiones tumorales en imágenes de ultrasonido. Estas imágenes suelen tener bajo contraste y presencia de ruido, lo que dificulta su interpretación. Mediante segmentación automática, el sistema aprende de ejemplos anotados a identificar patrones propios de los tumores. El resultado es una máscara que marca su ubicación y forma, lo cual puede acelerar y mejorar el diagnóstico clínico.

Para evaluar la calidad de la segmentación, el proyecto implementa métricas como el \textit{coeficiente de Dice} (DSC) y el \textit{Índice de Jaccard} (IoU), calculadas en el archivo \texttt{metrics\_evaluation.py}. Estas métricas comparan la superposición entre la predicción del modelo y la máscara real.

\subsection*{Arquitectura U-Net: Codificación, Decodificación y Conexiones de Salto}  
La arquitectura \textbf{U-Net} fue diseñada específicamente para segmentación de imágenes médicas. Tiene una forma en “U” compuesta por dos fases:

\begin{itemize}
  \item \textbf{Codificación (encoder)}: reduce progresivamente el tamaño de la imagen aplicando convoluciones y submuestreo (stride o pooling), extrayendo características globales.
  \item \textbf{Decodificación (decoder)}: reconstruye la resolución original mediante upsampling, refinando la predicción en cada paso.
\end{itemize}

Un elemento clave son las \textbf{conexiones de salto} (\textit{skip connections}), que copian los mapas de características del encoder y los concatenan con los del decoder en cada nivel. Esto permite recuperar detalles finos que se habrían perdido en el proceso de compresión, logrando una segmentación precisa, especialmente en bordes y estructuras pequeñas como los tumores.

\subsection*{Configuración Específica de U-Net empleada en el Proyecto}
El modelo se construye con la clase \texttt{UNet} de la librería MONAI, con la siguiente configuración:

\begin{itemize}
  \item \textbf{Espacio 2D}: \texttt{spatial\_dims=2}, adecuado para imágenes planas.
  \item \textbf{Canales}: Imagen de entrada en escala de grises (\texttt{in\_channels=1}) y salida binaria (\texttt{out\_channels=1}).
  \item \textbf{Estructura de capas}: Cuatro niveles con \texttt{channels=(16, 32, 64, 128)} y \texttt{strides=(2,2,2)} para reducir resolución.
  \item \textbf{Bloques residuales}: Se emplean bloques de dos convoluciones por nivel (\texttt{num\_res\_units=2}) que mejoran la estabilidad del entrenamiento.
  \item \textbf{Activaciones}: ReLU en capas intermedias; sigmoide al final para producir probabilidades píxel a píxel.
  \item \textbf{Función de pérdida}: \texttt{DiceLoss} con activación sigmoide incorporada.
  \item \textbf{Entrenamiento}: 15 épocas, batch size 2, optimizador Adam, con imágenes redimensionadas a 256x256 píxeles.
\end{itemize}

Este modelo, entrenado con imágenes del dataset BUSI y sus respectivas máscaras, se guarda como \texttt{unet\_busi.pt} y luego puede utilizarse para segmentar nuevas ecografías. La arquitectura, gracias a su simetría y al uso de conexiones de salto, ofrece segmentaciones precisas incluso en imágenes complejas y ruidosas como las ecografías mamarias.

\subsection*{Framework MONAI}
MONAI (Medical Open Network for AI) es una librería desarrollada para facilitar la implementación de modelos de aprendizaje profundo en el campo de la medicina. Extiende las funcionalidades de PyTorch con herramientas especializadas, incluyendo:

\begin{itemize}
    \item Transformaciones específicas para imágenes médicas (carga de datos, normalización, redimensionamiento).
    \item Modelos predefinidos como U-Net, V-Net y otros.
    \item Funciones de pérdida y métricas de evaluación adaptadas a segmentación.
    \item Integración con formatos de imagen médica (DICOM, NIfTI).
\end{itemize}

Gracias a MONAI, el desarrollo de este proyecto se beneficia de un entorno robusto y reproducible, permitiendo un enfoque modular y escalable para el tratamiento de imágenes médicas.


\section{Implementación}

La implementación de este proyecto se ha llevado a cabo usando únicamente el lenguaje de Python, utilizando las bibliotecas especializadas de OpenCV y NumPy pare el procesamiento de imágenes; scikit-image y SciPy para el análisis de texturas; y MONAI para el entrenamiento de redes neuronales profundas orientadas a imágenes médicas.
El flujo de trabajo se estructura de la siguiente forma
\begin{itemize}
    \item Preprocesamiento de las imágenes originales con filtros tradicionales y texturales.
    \item Fusión de canales para obtener imágenes en formato RGB enriquecidas.
    \item Segmentación automática de las imágenes mediante una red U-Net.
    \item Evaluación cuantitativa de los resultados usando métricas estándar como el coeficiente Dice, IoU, precisión, exactitud(accuracy), sensibilidad(recall) y especificidad.
\end{itemize}

A continuación se detallará el funcionamiento de cada parte del sistema, explicando a su vez  por qué se han tomado cada una de las decisiones y cómo se integran para resolver el problema de segmentación que nos encontramos tratando.

\subsection{Preprocesamiento de Imágenes}
Antes de aplicar técnicas avanzadas de análisis, las imágenes del dataset elegido pasan por la fase de preprocesamiento. Con esta fase lo que buscamos conseguir es mejorar la calidad visual de las imágenes a la vez que intentamos resaltar los detalles importantes, de forma que se le pueda facilitar el trabajo al modelo de segmentación. Este tratamiento se aplica a cada una de las imágenes del conjunto total por separado y se utilizan dos tipos de transformaciones diferentes: filtros tradicionales y extracción de características texturales. 

\subsubsection{\textit{Filtros tradicionales:}}
Los  \textbf{filtros tradicionales} que permiten mejorar aspectos básicos de las imágenes como el ruido, el contraste o los bordes. Se han usado dos tipos:
\begin{itemize}
    \item \textbf{Filtro de realce de bordes}(Butterworth): Se usa con el objetivo de resaltar las zonas con cambios bruscos de intensidad, queseparan distintas estructuras en las imágenes.
\begin{lstlisting}[caption={Filtro de Butterworth en frecuencia:}, label=lst:butter]
def butterworth_high_pass(img, cutoff=30, order=4):
    rows, cols = img.shape
    u = np.array(range(rows)) - rows / 2
    v = np.array(range(cols)) - cols / 2
    u, v = np.meshgrid(u, v, indexing='ij')
    D = np.sqrt(u ** 2 + v ** 2)
    H = 1 / (1 + (cutoff / (D + 1e-5)) ** (2 * order))
    img_fft = fft2(img)
    img_fft_shift = np.fft.fftshift(img_fft)
    img_filtered = img_fft_shift * H
    img_ifft = ifft2(np.fft.ifftshift(img_filtered))
    return np.abs(img_ifft).astype(np.uint8)
\end{lstlisting}
    \item \textbf{Filtro de mediana:} Con este filtro lo que se busca es limpiar la imagen sin distorsionar su contenido, ya que permite eliminar el ruido manteniendo las formas y los bordes.
    \begin{lstlisting}[caption={Filtro de mediana en Python}, label=lst:median]
    def adaptive_median_filter(img, kernel_size=5):
        return cv2.medianBlur(img, kernel_size)
    \end{lstlisting}

\end{itemize}
\subsubsection{\textit{Extracción de características texturales:}}
Además de los filtros básicos, se calculan características que ayudan a describir cómo cambia el color o la textura en distintas partes de la imagen. La extracción de características texturales realizada es:
\begin{itemize}
    \item \textbf{Características de primer orden}: Se calcula el brillo medio en regiones pequeñas de la imagen, que ayuda a suavizar la información. Además, también se calcula la entropía, que mide cuánto varían los tonos de gris en distintas zonas, indicando así el nivel de detalle o textura.  Con esto se consigue describir mejor el aspecto general de la imagen.
    \begin{lstlisting}[caption={Características de primer orden}, label=lst:firstorder]
    def first_order_features(img):
        mean = cv2.blur(img, (5, 5))
        ent = entropy(img, disk(5))
        ent = ((ent - ent.min()) / (ent.max() - ent.min()) * 255).astype(np.uint8)
        return mean, ent
    \end{lstlisting}

    \item \textbf{Características de segundo orden:}  Se detectan bordes finos y cambios rápidos de color o intensidad utilizando dos operadores sencillos: El detector de bordes de Canny, que localiza los contornos más marcados y el filtro Laplaciano, que resalta zonas donde la intensidad cambia rápidamente. Estos métodos ayudan a identificar detalles más finos en la imagen.
    \begin{lstlisting}[caption={Características de segundo orden}, label=lst:secondorder]
    def second_order_features_fallback(img):
        edges = cv2.Canny(img, 50, 150)
        laplacian = cv2.Laplacian(img, cv2.CV_64F)
        laplacian = np.uint8(np.absolute(laplacian))
        return edges, laplacian
    \end{lstlisting}
\end{itemize}
Con la aplicación de cada una de las transformaciones se generan nuevas versiones de cada imagen, que se combinan en una sola de tipo RGB (con tres canales de información), permitiéndole así al modelo trabajar con más información visual que la ofrecida por la imagen en blanco y negro original. 
\begin{lstlisting}[caption={Fusión de canales en RGB}, label=lst:fusion]
    def fuse_channels(ch1, ch2, ch3):
        return cv2.merge((ch1, ch2, ch3))
\end{lstlisting}
Además, se guardan imágenes comparativas que muestran los resultados del preprocesamiento, lo que facilita la validación e interpretación visual del proceso.
\begin{lstlisting}[caption={Guardado de comparativas visuales del preprocesamiento}, label=lst:comparativa]
    fig, axs = plt.subplots(1, 5, figsize=(20, 4))
    axs[0].imshow(img, cmap='gray'); axs[0].set_title("Original")
    axs[1].imshow(butter, cmap='gray'); axs[1].set_title("Butterworth")
    axs[2].imshow(median, cmap='gray'); axs[2].set_title("Median")
    axs[3].imshow(first_order_fused[..., ::-1]); axs[3].set_title("First-order")
    axs[4].imshow(second_order_fused[..., ::-1]); axs[4].set_title("Second-order")
    for ax in axs:
        ax.axis('off')
    plt.tight_layout()
    plt.savefig(f"{output_dir}/{base}_comparison.png")
    plt.close()
\end{lstlisting}
\subsubsection{\textit{Librerías utilizadas:}}
Para llevar a cabo el preprocesamiento de las imágenes se han utilizado distintas librerías especializadas en procesamiento de imágenes y análisis de datos:
\begin{itemize}
    \item \textbf{NumPy (numpy):}Se utiliza para realizar cálculos matriciales y manipular datos de las imágenes de manera eficiente.
    \item \textbf{OpenCV (cv2):} Se usa para: \begin{itemize} \item Leer imágenes en escala de grises. \item Aplicar filtros espaciales como el filtro de mediana. \item Redimensionar imágenes y fusionar canales en una imagen RGB. \item Guardar las imágenes procesadas en el disco. \end{itemize}
     \item \textbf{ scikit-image (skimage)} Se emplea para aplicar filtros más avanzados, como el cálculo de la entropía local de una imagen.
    \item \textbf{ Matplotlib (matplotlib):} Librería utilizada para visualizar y guardar comparativas de las imágenes preprocesadas.
     \item \textbf{ SciPy (scipy.fftpack):}Se utiliza para aplicar el filtro de Butterworth en el dominio de la frecuencia. Esta operación requiere realizar transformadas de Fourier y sus inversas, lo que se gestiona eficientemente con SciPy.
\end{itemize}
\subsection{Entrenamiento del modelo U-Net}
En esta siguiente fase del proyecto se entrena un modelo de red neuronal convolucional de tipo \textbf{U-Net}, una arquitectura especializada en la tarea de segmentación de imágenes. El objetivo de esta sección es que nuestro modelo aprenda a identificar automáticamente las regiones sospechosas en las imágenes ecográficas de mama.

\subsubsection{\textit{Librerías Utilizadas:}}
Para el desarrollo de esta parte del código hemos usado varias librerías de Python:
\begin{itemize}
    \item \textbf{NumPy (numpy)}
    \begin{itemize}
        \item \textit{¿Qué es?:} Librería fundamental para trabajar con matrices y realizar operaciones matemáticas de forma rápida y eficiente en Python.
    \end{itemize}
    \begin{itemize}
        \item \textit{¿Para qué se usa en esta sección del código?:} Para normalizar los valores de las imágenes, transformándolos de 0-255 a 0-1 y realizar operaciones básicas de manipulación de arrays (por ejemplo, conversión de tipos de datos).
    \end{itemize}
    
    \item \textbf{OpenCV (cv2)}
         \begin{itemize}
        \item \textit{¿Qué es?:}Librería usada para el procesamiento de imágenes.
    \end{itemize}
    \begin{itemize}
        \item \textit{¿Para qué se usa en esta sección del código?:} Para leer las imágenes y sus correspondientes máscaras desde archivos en formato PNG y para redimensionar las imágenes a un tamaño fijo que pueda ser procesado por la red neuronal (por ejemplo, 256x256 píxeles).
    \end{itemize}
    
     \item \textbf{ PyTorch (torch)}
         \begin{itemize}
        \item \textit{¿Qué es?:}Librería de aprendizaje profundo ampliamente utilizada para construir y entrenar redes neuronales.
    \end{itemize}
    \begin{itemize}
        \item \textit{¿Para qué se usa en esta sección del código?:} Para definir estructuras de datos (tensores) que representan las imágenes y máscaras, para crear la clase personalizada de conjunto de datos (BUSIDataset) que organiza las imágenes y máscaras; y para gestionar el entrenamiento del modelo: propagación hacia adelante, cálculo de la pérdida, retropropagación y actualización de pesos.
    \end{itemize}
    \item \textbf{ Matplotlib (matplotlib)}
         \begin{itemize}
        \item \textit{¿Qué es?:}Librería para crear gráficos y visualizar datos.
    \end{itemize}
    \begin{itemize}
        \item \textit{¿Para qué se usa en esta sección del código?:} Para guardar la imagenes comparativas.
    \end{itemize}
     \item \textbf{ glob y os (glob, os)}
         \begin{itemize}
        \item \textit{¿Qué son?:}
            \begin{itemize}
                \item \textit glob es un módulo que permite buscar archivos siguiendo patrones de nombre (por ejemplo, todos los archivos .png de una carpeta).
                \item \textit os es un módulo estándar de Python para trabajar con rutas de archivos y carpetas.
             \end{itemize}
    \end{itemize}
    \begin{itemize}
        \item \textit{¿Para qué se usa en esta sección del código?:} Para buscar automáticamente las imágenes y sus máscaras dentro de las carpetas de datos y crear carpetas donde se guardarán las predicciones y los resultados del modelo.
    \end{itemize}
    \item \textbf{ MONAI (monai)}
         \begin{itemize}
        \item \textit{¿Qué es?:}MONAI (Medical Open Network for AI) es una librería construida sobre PyTorch, especializada en el procesamiento y análisis de imágenes médicas. Ha sido desarrollada en colaboración con expertos médicos y de inteligencia artificial para facilitar el desarrollo de modelos de Deep Learning en el ámbito médico.
        \item \textit{¿Qué ofrece MONAI?:}Modelos predefinidos específicamente diseñados para imágenes médicas (por ejemplo, U-Net, V-Net, SegResNet). Funciones de pérdida específicas para segmentación médica (como DiceLoss). Métricas de evaluación adaptadas a problemas de segmentación (como DiceMetric). Transformaciones y utilidades específicas para imágenes de tomografías, resonancias magnéticas, ecografías, etc. Herramientas para asegurar resultados reproducibles.
        \end{itemize}
        \begin{itemize}
            \item \textit{¿Qué parte de MONAI estamos usando en este proyecto y para qué?:} 
                \begin{itemize}
                    \item \textit UNet: Se utiliza para construir directamente el modelo de red neuronal de segmentación.
                    \item \textit DiceLoss: Funciona como medida de error durante el entrenamiento, favoreciendo que las segmentaciones predichas se parezcan mucho a las reales.
                    \item \textit DiceMetric: Permite calcular la calidad de las segmentaciones de forma precisa tras cada época de entrenamiento.
                    \item \textit {$set_determinism$}: Se usa para asegurar que los resultados sean reproducibles, fijando una semilla aleatoria en las operaciones.
                \end{itemize}
        \item \textit{¿Por qué se ha elegido MONAI?:}Aunque PyTorch podría cubrir las necesidades básicas, MONAI simplifica enormemente el trabajo para tareas médicas:
            \begin{itemize}
                \item \textit Reduce el número de líneas de código necesarias.
                \item \textit Ofrece componentes optimizados y validados en aplicaciones médicas reales.
                \item \textit  Mejora la precisión y estabilidad de los modelos sin necesidad de implementaciones manuales complicadas.
            \end{itemize}
        Por tanto, gracias a Monai, podemos centrarnos más en el preprocesamiento, la evaluación de los resultados y la comparativa entre usar o no el preprocesamiento desarrollado que en programar de forma detallada la U-Net usada.
        \end{itemize}
\end{itemize}
\subsubsection{\textit{Preparación de los datos:}}
Primero, se recorre el conjunto de datos para encontrar las parejas imágenes y sus máscaras correspondientes. Las imágenes representan ecografías reales, y las máscaras son imágenes binarias que indican dónde se encuentra la zona a segmentar, en nuestro caso, un tumor. 
\begin{lstlisting}[caption={Buscar parejas de imágenes y máscaras}, label=lst:getpairs]
def get_image_mask_pairs():
    images, masks = [], []
    for folder in os.listdir(IMAGE_DIR):
        full_folder = os.path.join(IMAGE_DIR, folder)
        for path in glob.glob(os.path.join(full_folder, "*.png")):
            if "_mask" in path:
                continue
            base = os.path.splitext(os.path.basename(path))[0]
            mask_path = os.path.join(full_folder, base + "_mask.png")
            if os.path.exists(mask_path):
                images.append(path)
                masks.append(mask_path)
    return images, masks
\end{lstlisting}
Después de eso se define una clase personalizada (BUSIDataset) que,  al cargar cada imagen:
\begin{itemize}
    \item \textit La convierte a escala de grises.
    \item \textit La redimensiona a un tamaño fijo (256x256 píxeles).
    \item \textit La normaliza (valores entre 0 y 1).
    \item \textit Convierte la máscara a blanco y negro (binaria: tumor = 1, fondo = 0).
    \item \textit Devuelve ambos como tensores (un tensor es una estructura de datos similar a una matriz que permite realizar operaciones matemáticas rápidas y eficaces durante el entrenamiento (Pytorch, s.f.)) para que puedan ser procesados por la red neuronal.
\end{itemize}
\begin{lstlisting}[caption={Conjunto de datos personalizado BUSIDataset}, label=lst:dataset]
class BUSIDataset(TorchDataset):
    def __init__(self, image_paths, mask_paths):
        self.image_paths = image_paths
        self.mask_paths = mask_paths

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img = cv2.imread(self.image_paths[idx], cv2.IMREAD_GRAYSCALE)
        mask = cv2.imread(self.mask_paths[idx], cv2.IMREAD_GRAYSCALE)

        img = cv2.resize(img, IMG_SIZE)
        mask = cv2.resize(mask, IMG_SIZE)

        img = img.astype(np.float32) / 255.0
        mask = (mask.astype(np.float32) > 0).astype(np.float32)

        img_tensor = torch.tensor(img).unsqueeze(0)
        mask_tensor = torch.tensor(mask).unsqueeze(0)

        return {"img": img_tensor, "seg": mask_tensor}
\end{lstlisting}

\subsubsection{\textit{Definición del modelo U-Net:}}
Se construye una red neuronal convolucional de tipo \textbf{U-Net} usando MONAI. Se utiliza este tipo de arquitectura debido a que es ideal para segmentación, ya que (Ronneberger, O., Fischer, P., Brox, T.,2015): 
\begin{itemize}
    \item \textit Tiene una parte de compresión (encoder), que reduce el tamaño de la imagen para extraer características generales.
    \item \textit Y una parte de expansión (decoder), que recupera el tamaño original de la imagen y predice la máscara.
\end{itemize}
El modelo U-Net definido tiene:
\begin{itemize}
    \item \textit\textbf{ Entrada} de una imagen de un solo canal (escala de grises).
    \item \textit \textbf{Salida} de una imagen de un solo canal (máscara predicha binaria).
    \item \textit \textbf{Varios niveles} (4 niveles en este caso) que reducen y luego recuperan el tamaño de la imagen, extrayendo características y luego reconstruyendo la segmentación.
    \item \textit\textbf{ Capas internas} con distintos números de filtros (16, 32, 64 y 128).
\end{itemize}
\begin{lstlisting}[caption={Definición del modelo U-Net usando MONAI}, label=lst:unetmodel]
device = torch.device("cuda" if USE_CUDA else "cpu")

model = UNet(
    spatial_dims=2,
    in_channels=1,
    out_channels=1,
    channels=(16, 32, 64, 128),
    strides=(2, 2, 2),
    num_res_units=2
).to(device)
\end{lstlisting}


\subsubsection{\textit{Configuración del entrenamiento:}}
Una vez definido el modelo, se configura todo lo necesario para entrenarlo:
\begin{itemize}
    \item \textit\textbf \textbf{Función de pérdida (DiceLoss)}:Se usa para comparar la máscara predicha por el modelo con la real.
Cuanto más parecidas sean, menor será el error.
    \item \textit \textbf\textbf{ Optimizador (Adam)}:Ajusta automáticamente los parámetros del modelo para que mejore en cada paso.
    \item \textit \textbf \textbf{Métrica (DiceMetric):} Se calcula tras cada época para evaluar el rendimiento del modelo de forma cuantitativa.
\end{itemize}
\begin{lstlisting}[caption={Configuración de la pérdida, optimizador y métrica}, label=lst:trainconfig]
loss_fn = DiceLoss(sigmoid=True)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
metric = DiceMetric(include_background=False, reduction="mean")
\end{lstlisting}

\subsubsection{\textit{Proceso de entrenamiento:}}
El modelo se entrena durante varias iteraciones llamadas épocas.
En cada época:
\begin{itemize}
    \item \textit\textbf Se divide el conjunto de datos en pequeños grupos (lotes).
    \item \textit \textbf Se hace una predicción para cada lote.
    \item \textit \textbf Se calcula el error (pérdida) y se actualizan los parámetros del modelo.
    \item \textit\textbf Se guarda el error total para poder seguir la evolución del aprendizaje.
\end{itemize}
Este proceso se repite durante 15 épocas (aunque este número puede cambiarse).
\begin{lstlisting}[caption={Bucle principal de entrenamiento}, label=lst:trainloop]
for epoch in range(EPOCHS):
    model.train()
    epoch_loss = 0
    for batch in loader:
        x = batch["img"].to(device)
        y = batch["seg"].to(device)

        optimizer.zero_grad()
        y_pred = model(x)
        loss = loss_fn(y_pred, y)
        loss.backward()
        optimizer.step()

        epoch_loss += loss.item()

    print(f"Epoch {epoch+1}/{EPOCHS}, Loss: {epoch_loss/len(loader):.4f}")
\end{lstlisting}

\subsubsection{\textit{Guardado del modelo:}}
Cuando termina el entrenamiento, se guarda el modelo entrenado en un archivo llamado \texttt{unet\_busi.pt}.
Este archivo se puede usar posteriormente para hacer predicciones sobre nuevas imágenes sin necesidad de volver a entrenar.
\begin{lstlisting}[caption={Guardado del modelo}, label=lst:save_and_viz]
torch.save(model.state_dict(), "unet_busi.pt")
print("\n Modelo UNet entrenado y guardado como 'unet_busi.pt'")

\end{lstlisting}

\subsubsection{\textit{Visualización de resultados:}}
Como verificación rápida, el sistema genera imágenes comparativas para algunas predicciones realizadas sobre el conjunto de entrenamiento.
Estas imágenes muestran:
\begin{itemize}
    \item \textit\textbf La imagen original.
    \item \textit \textbf La máscara real.
    \item \textit \textbf La máscara predicha por el modelo.
\end{itemize}
Esto permite comprobar visualmente si el modelo está segmentando correctamente.
\begin{lstlisting}[caption={Generación de comparativas}, label=lst:save_and_viz]
model.eval()
os.makedirs(OUTPUT_DIR, exist_ok=True)
with torch.no_grad():
    for i, batch in enumerate(loader):
        x = batch["img"].to(device)
        y = batch["seg"].to(device)
        pred = torch.sigmoid(model(x)) > 0.5

        for j in range(x.shape[0]):
            img_np = x[j][0].cpu().numpy()
            mask_np = y[j][0].cpu().numpy()
            pred_np = pred[j][0].cpu().numpy()

            fig, axs = plt.subplots(1, 3, figsize=(10, 3))
            axs[0].imshow(img_np, cmap="gray")
            axs[0].set_title("Imagen")
            axs[1].imshow(mask_np, cmap="gray")
            axs[1].set_title("Máscara Real")
            axs[2].imshow(pred_np, cmap="gray")
            axs[2].set_title("Predicción")

            for ax in axs:
                ax.axis('off')

            plt.tight_layout()
            plt.savefig(f"{OUTPUT_DIR}/comparison_{i}_{j}.png")
            plt.close()

        if i >= 4:
            break
\end{lstlisting}
\subsection{Fase de predicción}
\subsubsection{\textit{Librerías Utilizadas:}}
Las librerías utilizadas en esta fase son prácticamente las mismas que en el módulo de entrenamiento, por lo que no se volverán a explicar en detalle.
Cabe destacar que aquí se hace uso de la función torch.sigmoid para convertir la salida del modelo en probabilidades entre 0 y 1, lo cual permite generar máscaras binarias a partir de las predicciones.

\subsubsection{\textit{Carga del modelo y configuración del dispositivo:}}
Antes de hacer predicciones, se carga el modelo previamente entrenado desde el archivo \texttt{unet\_busi.pt}. Además, se detecta si hay una GPU disponible y, si no, se utiliza la CPU. Esto asegura que el modelo funcione en cualquier equipo.
\begin{lstlisting}[caption={Carga del modelo y selección de dispositivo}, label=lst:predict_device_model]
device = torch.device("cuda" if USE_CUDA else "cpu")

model = UNet(
    spatial_dims=2,
    in_channels=1,
    out_channels=1,
    channels=(16, 32, 64, 128),
    strides=(2, 2, 2),
    num_res_units=2
).to(device)

model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
model.eval()
\end{lstlisting}
\subsubsection{\textit{Lectura de imágenes para predecir:}}
Se buscan todas las imágenes en la carpeta de test (sin sus máscaras).
El sistema ignora cualquier archivo que ya contenga la palabra mask en su nombre.
Se prepara una carpeta para guardar las predicciones.
\begin{lstlisting}[caption={Búsqueda de imágenes nuevas para predecir}, label=lst:predict_images_input]
os.makedirs(OUTPUT_DIR, exist_ok=True)

image_paths = glob.glob(os.path.join(IMAGE_DIR, "**", "*.png"), recursive=True)
image_paths = [p for p in image_paths if not p.endswith("_mask.png")]
print(f"Encontradas {len(image_paths)} imágenes para predecir.")
\end{lstlisting}
\subsubsection{\textit{Proceso de predicción y guardado de resultados:}}
Para cada imagen:
\begin{itemize}
    \item \textit\textbf Se convierte a escala de grises.
    \item \textit \textbf Se redimensiona y normaliza.
    \item \textit \textbf Se transforma en tensor para que el modelo pueda procesarla.
    \item \textit \textbf El modelo genera una máscara binaria (predicción).
    \item \textit \textbf Se guarda esta máscara como imagen .png.
\end{itemize}

\begin{lstlisting}[caption={Predicción y guardado de la máscara generada}, label=lst:predict_loop]
with torch.no_grad():
    for i, img_path in enumerate(image_paths):
        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        if img is None:
            print(f"Error cargando: {img_path}")
            continue

        img_resized = cv2.resize(img, IMG_SIZE)
        img_norm = img_resized.astype(np.float32) / 255.0
        img_tensor = torch.tensor(img_norm).unsqueeze(0).unsqueeze(0).to(device)

        pred = torch.sigmoid(model(img_tensor))
        pred_mask = (pred[0, 0].cpu().numpy() > 0.5).astype(np.uint8) * 255

        filename = os.path.basename(img_path)
        filename = filename.replace(" ", "_").replace("(", "").replace(")", "")
        output_path = os.path.join(OUTPUT_DIR, f"mask_{filename}")
        cv2.imwrite(output_path, pred_mask)
\end{lstlisting}
\subsubsection{\textit{Generación de comparativas visuales:}}
Para facilitar la revisión visual de los resultados, se genera una imagen que muestra:
\begin{itemize}
    \item \textit\textbf La imagen original.
    \item \textit \textbf La máscara predicha por el modelo.
\end{itemize}
Esto se guarda como un archivo PNG en la misma carpeta de salida.
\begin{lstlisting}[caption={Comparativa visual de la predicción}, label=lst:predict_comparison]
        fig, axs = plt.subplots(1, 2, figsize=(8, 4))
        axs[0].imshow(img_resized, cmap="gray")
        axs[0].set_title("Imagen original")
        axs[1].imshow(pred_mask, cmap="gray")
        axs[1].set_title("Predicción máscara")
        for ax in axs:
            ax.axis('off')
        plt.tight_layout()
        plt.savefig(os.path.join(OUTPUT_DIR, f"comp_{filename}"))
        plt.close()

        print(f"Predicción guardada para: {filename}")
\end{lstlisting}
\subsection{Evaluación de métricas }
\subsubsection{\textit{Comparación entre máscaras reales y predichas:}}
Primero se recorren todos los archivos de predicción generados.
Para cada uno, se busca su máscara real correspondiente usando el nombre del archivo.
Esto permite comparar la predicción con la verdad, imagen por imagen.
\begin{lstlisting}[caption={Recorrer predicciones y obtener la GT}, label=lst:metrics_compare]
pred_files = [f for f in os.listdir(pred_dir) if f.startswith("mask_") and f.endswith(".png")]

for pred_file in pred_files:
    try:
        gt_filename = get_gt_filename(pred_file)
        if not gt_filename:
            print(f"Nombre de predicción no válido: {pred_file}")
            continue

        class_name = gt_filename.split()[0]
        gt_path = os.path.join(gt_dir, class_name, gt_filename)
        pred_path = os.path.join(pred_dir, pred_file)

        if not os.path.exists(gt_path):
            print(f"No se encontró GT para {pred_file}")
            continue

        gt_mask = cv2.imread(gt_path, cv2.IMREAD_GRAYSCALE)
        pred_mask = cv2.imread(pred_path, cv2.IMREAD_GRAYSCALE)
\end{lstlisting}
\subsubsection{\textit{Preparación y binarización de máscaras:}}
Antes de comparar las máscaras, se convierten a imágenes binarias:
\begin{itemize}
    \item \textit\textbf Zonas con valor alto (blanco) = 1.
    \item \textit \textbf Zonas oscuras (fondo) = 0.
\end{itemize}
También se redimensiona la máscara predicha si no coincide en tamaño con la real.
\begin{lstlisting}[caption={Normalización y binarización de máscaras}, label=lst:metrics_binarize]
        if pred_mask.shape != gt_mask.shape:
            pred_mask = cv2.resize(pred_mask, (gt_mask.shape[1], gt_mask.shape[0]), interpolation=cv2.INTER_NEAREST)

        gt_mask = (gt_mask > 127).astype(np.uint8)
        pred_mask = (pred_mask > 127).astype(np.uint8)
\end{lstlisting}
\subsubsection{\textit{Cálculo de métricas de evaluación:}}
Se calculan varias métricas para medir cuán buena es la segmentación del modelo:
\begin{itemize}
    \item \textit\textbf \textbf{Dice Coefficient (DSC):} mide la superposición entre las máscaras.
    \item \textit \textbf \textbf{IoU (Intersection over Union):} mide cuánto coinciden las zonas predichas y reales.
    \item \textit \textbf \textbf{Accuracy, Precision, Recall (Sensibilidad) y Specificity:} métricas clásicas en clasificación binaria, adaptadas aquí a la segmentación.
\end{itemize}
\begin{lstlisting}[caption={Cálculo de métricas de segmentación}, label=lst:metrics_calc]
        dice = dice_score(gt_mask, pred_mask)
        iou = iou_score(gt_mask, pred_mask)
        acc = accuracy_score(gt_mask.flatten(), pred_mask.flatten())
        prec = precision_score(gt_mask.flatten(), pred_mask.flatten(), zero_division=0)
        rec = recall_score(gt_mask.flatten(), pred_mask.flatten(), zero_division=0)
        spec = specificity_score(gt_mask, pred_mask)

        dice_list.append(dice)
        iou_list.append(iou)
        acc_list.append(acc)
        prec_list.append(prec)
        rec_list.append(rec)
        spec_list.append(spec)
\end{lstlisting}
\subsubsection{\textit{Mostrar resultados finales:}}
Una vez comparadas todas las imágenes, se muestra la media de todas las métricas, lo que permite evaluar el rendimiento general del modelo.
\begin{lstlisting}[caption={Mostrar métricas medias finales}, label=lst:metrics_result]
print("\nEvaluación final (predicciones válidas):")
print(f"Dice Coefficient (DSC): {np.mean(dice_list):.4f}")
print(f"IoU: {np.mean(iou_list):.4f}")
print(f"Accuracy: {np.mean(acc_list):.4f}")
print(f"Precision: {np.mean(prec_list):.4f}")
print(f"Recall (Sensitivity): {np.mean(rec_list):.4f}")
print(f"Specificity: {np.mean(spec_list):.4f}")
\end{lstlisting}


\section{Experimentación}

\subsection{Objetivo de la experimentación}
El objetivo principal es evaluar el impacto de diferentes técnicas de \textbf{preprocesamiento} sobre el rendimiento de un modelo de segmentación de tumores en imágenes ecográficas. Se compara el rendimiento de la red U-Net entrenada con imágenes originales frente a aquellas que han sido sometidas a distintos métodos de realce y filtrado de textura.

\subsection{Configuración experimental}
Se ha utilizado el conjunto de datos \textbf{BUSI} (Breast Ultrasound Images), que contiene imágenes clasificadas en tres categorías: benignas, malignas y normales. La arquitectura empleada para la segmentación ha sido \textbf{U-Net}, entrenada con las siguientes condiciones constantes:
\begin{itemize}
    \item \textbf{Tamaño de entrada:} 256 × 256 píxeles
    \item \textbf{Épocas:} 30
    \item \textbf{Función de pérdida:} DiceLoss
    \item \textbf{Evaluación:} sobre imágenes originales y preprocesadas
\end{itemize}
La variable experimental es el \textbf{tipo de preprocesamiento} aplicado a las imágenes.

\subsection{Técnicas de preprocesamiento}
Se han evaluado cinco configuraciones distintas:
\begin{itemize}
    \item \textbf{Sin preprocesamiento:} imágenes originales del dataset.
    \item \textbf{Butterworth:} filtro pasa-altos en el dominio de la frecuencia, diseñado para resaltar bordes sin introducir ruido abrupto.
    \item \textbf{Filtro mediana adaptativa:} reduce el ruido tipo speckle característico de las imágenes ecográficas sin perder detalles de bordes.
    \item \textbf{Textura de primer orden:} basada en la media y entropía de vecindades locales, para representar intensidad y aleatoriedad.
    \item \textbf{Textura de segundo orden:} utiliza operadores como Canny y Laplaciano para aproximar características de bordes, autocorrelación y homogeneidad.
\end{itemize}

Las texturas se combinan con la imagen original formando una imagen \textbf{RGB tridimensional}, en la que cada canal representa una característica diferente (ej. canal rojo = original, verde = textura1, azul = textura2).

\subsection{Métricas de evaluación}
Se utilizan las siguientes métricas para evaluar el rendimiento del modelo:
\begin{itemize}
    \item \textbf{Dice Coefficient (DSC):} mide la superposición entre la máscara predicha y la real. Cuanto más alto, mejor (ideal: 1).
    \item \textbf{IoU (Intersection over Union):} relación entre la intersección y la unión de las máscaras predicha y real.
    \item \textbf{Accuracy:} proporción de píxeles correctamente clasificados (positivos y negativos).
    \item \textbf{Precision:} proporción de verdaderos positivos entre todos los positivos predichos.
    \item \textbf{Recall (Sensibilidad):} capacidad del modelo para detectar todos los píxeles positivos reales.
    \item \textbf{Specificity:} capacidad para identificar correctamente los píxeles negativos.
\end{itemize}

\subsection{Resultados}
Los resultados medios obtenidos para cada técnica son:

\begin{center}
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
\textbf{Técnica} & \textbf{DSC} & \textbf{IoU} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{Specificity} \\
\hline
Sin preprocesamiento & 0.63  & 0.53  & 0.93     & 0.63      & 0.68   & 0.96        \\
Butterworth          & 0.6312& 0.5314& 0.9421   & 0.6361    & 0.6848 & 0.9635      \\
Mediana              & 0.6779& 0.5970& 0.9457   & 0.6926    & 0.6919 & 0.9605      \\
Primer orden         & 0.6635& 0.5764& 0.9498   & 0.7043    & 0.6578 & 0.9739      \\
Segundo orden        & 0.6607& 0.5671& 0.9321   & 0.6357    & 0.7202 & 0.9456      \\
\hline
\end{tabular}
\end{center}

También se han generado gráficas comparativas para visualizar el rendimiento de cada técnica en cada métrica, destacando la técnica ganadora por métrica.

\subsection{Análisis y conclusiones}
\begin{itemize}
    \item \textbf{Mejor DSC e IoU:} el filtro de mediana muestra la mejor superposición entre predicción y ground truth.
    \item \textbf{Mayor precisión:} textura de primer orden, aunque con un leve descenso en sensibilidad.
    \item \textbf{Mejor sensibilidad:} segundo orden, indicando buena detección de píxeles positivos.
    \item \textbf{Mejor especificidad y accuracy:} primer orden, que destaca en clasificación correcta de píxeles negativos.
\end{itemize}

En general, los preprocesamientos \textbf{mejoran el rendimiento respecto a usar imágenes originales}, siendo la \textbf{mediana adaptativa} y la \textbf{textura de primer orden} las más equilibradas en métricas clave.


\subsection*{5.Manual de Usuario}
\subsubsection*{5.1 Requisitos del sistema}
Para ejecutar el proyecto correctamente es necesario disponer de:

\begin{itemize}
    \item \textbf{Python 3.8} o superior.
    \item \textbf{Sistema operativo}: Windows, Linux o macOS.
    \item \textbf{Hardware recomendado}:
    \begin{itemize}
        \item CPU moderna.
        \item (Opcional) GPU compatible con CUDA para acelerar el entrenamiento (recomendado).
    \end{itemize}
\end{itemize}

\subsubsection*{5.2. Instalación de dependencias}

Antes de ejecutar el código, se deben instalar las librerías necesarias.  
Desde la carpeta raíz del proyecto, ejecutar el siguiente comando:

\begin{lstlisting}[language=bash]
pip install -r requirements.txt
\end{lstlisting}

Esto instalará automáticamente las dependencias necesarias:
\begin{itemize}
    \item numpy
    \item opencv-python
    \item matplotlib
    \item scikit-image
    \item scipy
    \item torch
    \item monai
\end{itemize}

\textbf{Nota}: Si se dispone de una GPU y se desea aprovechar, se recomienda instalar \texttt{torch} con soporte CUDA siguiendo las instrucciones de \url{https://pytorch.org/}.

\subsubsection*{5.3. Estructura del proyecto}

\begin{itemize}
    \item \texttt{main.py}: Script principal para lanzar las distintas fases del proyecto.
    \item \texttt{preprocessing.py}: Funciones de preprocesamiento de imágenes.
    \item \texttt{train\_unet\_monai.py}: Entrenamiento del modelo U-Net.
    \item \texttt{predict\_with\_unet.py}: Predicción de máscaras sobre nuevas imágenes.
    \item \texttt{metrics\_evaluation.py}: Evaluación de las predicciones mediante métricas.
    \item \texttt{unet\_busi.pt}: Archivo que guarda el modelo U-Net entrenado.
\end{itemize}

\subsubsection*{5.4. Cómo utilizar el proyecto}

\subsubsection*{5.4.1. Preprocesamiento de imágenes}

Para aplicar los filtros y generar imágenes procesadas, ejecutar:

\begin{lstlisting}[language=bash]
python main.py classic
\end{lstlisting}

Esto aplicará:
\begin{itemize}
    \item Filtro de Butterworth.
    \item Filtro de mediana.
    \item Extracción de características de primer orden (media y entropía).
    \item Extracción de características de segundo orden (bordes y cambios locales).
\end{itemize}

Los resultados se guardarán en la carpeta de salida especificada.

\subsubsection*{5.4.2. Entrenamiento del modelo U-Net}

Para entrenar el modelo desde cero, ejecutar:

\begin{lstlisting}[language=bash]
python main.py train
\end{lstlisting}

Este comando:
\begin{itemize}
    \item Preparará el conjunto de datos (imágenes y máscaras).
    \item Definirá y entrenará el modelo U-Net.
    \item Guardará el modelo entrenado en el archivo \texttt{unet\_busi.pt}.
\end{itemize}

\subsubsection*{5.4.3. Predicción con el modelo entrenado}

Para realizar predicciones sobre nuevas imágenes:

\begin{lstlisting}[language=bash]
python main.py predict
\end{lstlisting}

Esto:
\begin{itemize}
    \item Cargará el modelo entrenado.
    \item Aplicará la predicción a nuevas imágenes.
    \item Guardará las máscaras predichas y comparativas visuales.
\end{itemize}

\subsubsection*{5.4.4. Evaluación de resultados}

Para evaluar la calidad de las predicciones:

\begin{lstlisting}[language=bash]
python main.py evaluate
\end{lstlisting}

Se calcularán automáticamente las métricas:
\begin{itemize}
    \item Dice Coefficient (DSC)
    \item Intersection over Union (IoU)
    \item Accuracy
    \item Precision
    \item Recall (Sensibilidad)
    \item Specificity
\end{itemize}

\subsection*{5.5. Notas importantes}

\begin{itemize}
    \item Asegurarse de que las rutas de carpetas de entrada y salida estén correctamente configuradas.
    \item Si se desea modificar parámetros (por ejemplo, número de épocas), deben ajustarse en \texttt{train\_unet\_monai.py}.
    \item El proyecto detecta automáticamente si hay una GPU disponible.
    \item Los nombres de archivos deben seguir la convención: las máscaras deben tener el mismo nombre que su imagen asociada, añadiendo "\_mask" al final.
\end{itemize}


\section{Conclusiones}
Los resultados evidencian que las características de textura aumentan la diferenciación de tumores frente al fondo, mejorando la precisión de segmentación. La segunda orden destaca por resaltar bordes y contornos relevantes. Las métricas mejoran considerablemente frente a los métodos tradicionales.

\section{Autoevaluacion de cada miembro}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Evalúa} & \textbf{Juan} & \textbf{Mar} & \textbf{Nerea} \\
\hline
Juan & - & - & - \\
Mar & - & - & - \\
Nerea & - & - & - \\
\hline
\end{tabular}
\caption{Evaluación entre compañeros}
\end{table}

\section{Tabla de tiempos}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Tarea} & \textbf{Juan} & \textbf{Mar} & \textbf{Nerea} \\
\hline
Tarea 1 & - & - & - \\
Tarea 2 & - & - & - \\
Tarea 3 & - & - & - \\
Tarea 4 & - & - & - \\
Tarea 5 & - & - & - \\
Tarea 6 & - & - & - \\
\hline
\end{tabular}
\caption{Tiempo dedicado a cada tarea}
\end{table}

\section*{Bibliografía}
\begin{enumerate}
    \item S. Cai et al., "A Study on the Combination of Image Preprocessing Method Based on Texture Feature and Segmentation Algorithm for Breast Ultrasound Images", IEEE ICCECE, 2022.
    \item Yap, M. H. et al., "Automated breast ultrasound lesions detection using convolutional neural networks", IEEE JBHI, 2017.
    \item Ibtehaz, N. and Rahman, M. S., "MultiResUNet: Rethinking the U-Net architecture", Neural Networks, 2020.
    \item PyTorch. (s.f.). Tensors [Documentación]. Recuperado de \href{https://pytorch.org/docs/stable/tensors.html}{pytorch}
    \item Ronneberger, O., Fischer, P.,  Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. In Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015
\end{enumerate}

\end{document}